[
["index.html", "Modelos Predictivos Bienvenido Estructura del libro Software y convenciones Bloques informativos", " Modelos Predictivos Freddy Hernández Olga Usuga Carmen Patiño 2019-08-05 Bienvenido Este libro está destinado para estudiantes de ingeniería y estadística que deseen aprender sobre modelos de regresión y la forma de aplicarlos por medio del lenguaje de programación R. Freddy Hernández Olga Usuga Carmen Patiño Estructura del libro En el capítulo 1 se presentan los árboles de regresión. Software y convenciones Para realizar este libro usamos los paquetes knitr (Xie 2015) y bookdown (Xie 2018) que permiten unir la ventajas de LaTeX y R en un mismo archivo. En todo el libro se presentarán códigos que el lector puede copiar y pegar en su consola de R para obtener los mismos resultados aquí del libro. Los códigos se destacan en una caja de color similar a la mostrada a continuación. 4 + 6 a &lt;- c(1, 5, 6) 5 * a 1:10 Los resultados o salidas obtenidos de cualquier código se destacan con dos símbolos de númeral (##) al inicio de cada línea o renglón, esto quiere decir que todo lo que inicie con ## son resultados obtenidos y NO los debe copiar. Abajo se muestran los resultados obtenidos luego de correr el código anterior. ## [1] 10 ## [1] 5 25 30 ## [1] 1 2 3 4 5 6 7 8 9 10 Bloques informativos En varias partes del libro usaremos bloques informativos para resaltar algún aspecto importante. Abajo se encuentra un ejemplo de los bloques y su significado. Nota aclaratoria. Sugerencia. Advertencia. References "],
["arb-de-regre.html", "1 Árboles de regresión Ejemplo", " 1 Árboles de regresión Los árboles de regresión son árboles de decisión para asignar un valor de \\(\\hat{y}\\) dependiendo de los valores de las covariables. Para conocer más sobre el tema se recomienda ver este video. El paquete rpart (Therneau and Atkinson 2018) es uno de los paquetes que se pueden usar para crear árboles de regresión. La función para crear un árbol de regresión es rpart, a continuación la estructura de la función. rpart(formula, data, weights, subset, na.action = na.rpart, method, model = FALSE, x = FALSE, y = TRUE, parms, control, cost, ...) Ejemplo En este ejemplo se busca encontrar un modelo de regresion lineal que explique la variable respuesta \\(y\\) en función de las covariables \\(x_1\\) a \\(x_{11}\\), los datos provienen del ejercicio 9.5 del libro de Montgomery, Peck and Vining (2003). El paquete MPV (Braun 2018) contiene todos los datos que acompañan al libro. A continuación se muestra el encabezado de la base de datos y la definición de las variables. Figure 1.1: Ilustración de los métodos Forward y Backward. Nota: Type of transmission (1=automatic, 0=manual). Antes de iniciar es necesario revisar si hay NA's y eliminarlos. library(MPV) # Aqui estan los datos table.b3[22:26, ] # Can you see the missing values? ## y x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 ## 22 21.47 360.0 180 290 8.4 2.45 2 3 214.2 76.3 4250 1 ## 23 16.59 400.0 185 NA 7.6 3.08 4 3 196.0 73.0 3850 1 ## 24 31.90 96.9 75 83 9.0 4.30 2 5 165.2 61.8 2275 0 ## 25 29.40 140.0 86 NA 8.0 2.92 2 4 176.4 65.4 2150 0 ## 26 13.27 460.0 223 366 8.0 3.00 4 3 228.0 79.8 5430 1 datos &lt;- table.b3[-c(23, 25), ] El objeto datos tiene la base de datos sin las líneas con NA, lo mismo se hubiese podido realizar usando la función na.omit. La base de datos tiene 30 filas y 12 columnas. library(rpart) library(rpart.plot) mod &lt;- rpart(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11, data=datos) Dibjuemos el árbol con prp que es una función del paquete rpart.plot (Milborrow 2019). prp(mod) Construyamos nuevamente el árbol pero explorando todas las opciones de la función prp. prp(mod, main=&quot;&quot;, nn = TRUE, # display the node numbers fallen.leaves = TRUE, # put the leaves on the bottom of the page shadow.col = &quot;gray&quot;, # shadows under the leaves branch.lty = 3, # draw branches using dotted lines branch = .5, # change angle of branch lines faclen = 0, # faclen = 0 to print full factor names trace = 1, # print the auto calculated cex, xlim, ylim split.cex = 1.2, # make the split text larger than the node text split.prefix = &quot;is &quot;, # put &quot;is &quot; before split text split.suffix = &quot;?&quot;, # put &quot;?&quot; after split text split.box.col = &quot;lightblue&quot;, # lightgray split boxes (default is white) split.border.col = &quot;darkgray&quot;, # darkgray border on split boxes split.round = 0.5) # round the split box corners a tad ## cex 1 xlim c(0, 1) ylim c(0, 1) Usando la información del árbol anterior es posible predecir el valor de \\(y\\). Por ejemplo: Si una nueva observación tiene \\(x_9=70\\) y \\(x_2=100\\), entonces \\(\\hat{y}=20\\). Si otra observación tiene \\(x_9=60\\) y \\(x_2=150\\), entonces \\(\\hat{y}=28\\). Como en el árbol anterior solo aparecen las variables \\(x_2\\) y \\(x_9\\) se recomienda volver a construir el árbol sólo con ellas. mod &lt;- rpart(y ~ x2 + x9, data=datos) Para predecir los valores de \\(y\\) se puede usar la función predict. A continuación el código para predecir la respuesta en los dos casos anteriores. nuevos_datos &lt;- data.frame(x2=c(100, 150), x9=c(70, 60)) predict(object=mod, newdata=nuevos_datos) ## 1 2 ## 19.66875 28.06625 En este ejemplo los datos originales se usaron como conjunto de entrenamiento y prueba debido a que solo se cuentan con 30 observaciones. Entre más cerca estén las \\(\\hat{y}\\) de los \\(y\\) observados se puede decir que el modelo es mejor. A continuación la correlación entre \\(\\hat{y}\\) y \\(y\\). y_hat &lt;- predict(object=mod, newdata=datos) cor(y_hat, datos$y) ## [1] 0.8300304 ¿Qué opina de este valor? A continuación un diagrama de dispersión entre \\(\\hat{y}\\) y \\(y\\). plot(x=datos$y, y=y_hat, pch=20, las=1, xlab=&#39;y&#39;, ylab=expression(hat(y))) abline(a=0, b=1, lty=&quot;dashed&quot;, col=&quot;blue&quot;) References "],
["reg-versus-arb.html", "2 Regresión lineal versus árboles de regresión 2.1 Regresión lineal 2.2 Arboles de regresión Ejemplo 2.3 Estudio de simulación para comparar ambos métodos 2.4 Retos", " 2 Regresión lineal versus árboles de regresión En este capítulo se muestra una comparación entre modelos de regresión y árboles de regresion. 2.1 Regresión lineal El modelo de regresión lineal simple es uno de los más populares en modelación. Este modelo se puede resumir a continuación. \\[\\begin{align} y_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 x_i, \\\\ \\sigma^2 &amp;= \\text{constante} \\end{align}\\] 2.2 Arboles de regresión Una explicación de los árboles de regresión puede ser consultada en el capítulo 1. Las librerías en R para implementar árboles de regresión son: library(rpart) library(rpart.plot) Ejemplo Como ilustración vamos a usar los datos del ejemplo 2.1 del libro de Montgomery, Peck and Vining (2003). En el ejemplo 2.1 los autores ajustaron un modelo de regresión lineal simple para explicar la Resistencia de una soldadura en función de la Edad de la misma. A continuación el código para cargar los datos y una muestra de las 6 primeras observaciones de la base de datos, en total tenemos 20 observaciones. file &lt;- &quot;https://raw.githubusercontent.com/fhernanb/datos/master/propelente&quot; datos &lt;- read.table(file=file, header=TRUE) head(datos) # shows the first 6 rows ## Resistencia Edad ## 1 2158.70 15.50 ## 2 1678.15 23.75 ## 3 2316.00 8.00 ## 4 2061.30 17.00 ## 5 2207.50 5.50 ## 6 1708.30 19.00 Para crear un diagrama de dispersión que nos muestre la relación entre las dos variables usamos las siguientes instrucciones. library(ggplot2) ggplot(datos, aes(x=Edad, y=Resistencia)) + geom_point() + theme_light() De la figura anterior se ve claramente que a medida que aumenta la edad de la soldadura, la resistencia que ella ofrece disminuye. Adicionalmente, se observa que la relación entre las variables es lineal con una dispersión que parece constante. ¿Quién estima mejor? ¿un modelo de regresión lineal simple o un árbol? rls &lt;- lm(Resistencia ~ Edad, data=datos) arb &lt;- rpart(Resistencia ~ Edad, data=datos) arb &lt;- rpart(Resistencia ~ Edad, data=datos, method=&quot;anova&quot;) ¿Qué hay dentro de modelo de regresión lineal simple? summary(rls) ## ## Call: ## lm(formula = Resistencia ~ Edad, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -215.98 -50.68 28.74 66.61 106.76 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2627.822 44.184 59.48 &lt; 2e-16 *** ## Edad -37.154 2.889 -12.86 1.64e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 96.11 on 18 degrees of freedom ## Multiple R-squared: 0.9018, Adjusted R-squared: 0.8964 ## F-statistic: 165.4 on 1 and 18 DF, p-value: 1.643e-10 ¿Qué hay dentro de modelo del arbol? summary(arb) ## Call: ## rpart(formula = Resistencia ~ Edad, data = datos, method = &quot;anova&quot;) ## n= 20 ## ## CP nsplit rel error xerror xstd ## 1 0.7480619 0 1.0000000 1.106601 0.2384385 ## 2 0.0100000 1 0.2519381 1.106601 0.2384385 ## ## Variable importance ## Edad ## 100 ## ## Node number 1: 20 observations, complexity param=0.7480619 ## mean=2131.358, MSE=84686.88 ## left son=2 (8 obs) right son=3 (12 obs) ## Primary splits: ## Edad &lt; 16.25 to the right, improve=0.7480619, (0 missing) ## ## Node number 2: 8 observations ## mean=1823.094, MSE=19439.95 ## ## Node number 3: 12 observations ## mean=2336.867, MSE=22599.79 Construyamos nuevamente el árbol pero explorando todas las opciones de la función prp. prp(arb, main = &quot;&quot;, nn = TRUE, # display the node numbers fallen.leaves = TRUE, # put the leaves on the bottom of the page shadow.col = &quot;gray&quot;, # shadows under the leaves branch.lty = 3, # draw branches using dotted lines branch = .5, # change angle of branch lines faclen = 0, # faclen = 0 to print full factor names trace = 1, # print the auto calculated cex, xlim, ylim split.cex = 1.2, # make the split text larger than the node text split.prefix = &quot;is &quot;, # put &quot;is &quot; before split text split.suffix = &quot;?&quot;, # put &quot;?&quot; after split text split.box.col = &quot;lightblue&quot;, # lightgray split boxes (default is white) split.border.col = &quot;darkgray&quot;, # darkgray border on split boxes split.round = 0.5) # round the split box corners a tad ## cex 1 xlim c(-0.65, 1.65) ylim c(-0.15, 1.15) A continuación las predicciones con ambos modelos. pred_rls &lt;- predict(object=rls, newdata=datos) pred_arb &lt;- predict(object=arb, newdata=datos) Dibujemos \\(y_i\\) versus \\(\\hat{y}_i\\). par(mfrow=c(1, 2)) plot(x=pred_rls, y=datos$Resistencia, main=&quot;RLS&quot;) abline(a=0, b=1, lty=&quot;dashed&quot;, col=&quot;blue&quot;) plot(x=pred_arb, y=datos$Resistencia, main=&quot;Arbol&quot;) abline(a=0, b=1, lty=&quot;dashed&quot;, col=&quot;blue&quot;) Vamos a calcular \\(Cor(y_i, \\hat{y}_i)\\). cor(datos$Resistencia, pred_rls) ## [1] 0.9496533 cor(datos$Resistencia, pred_arb) ## [1] 0.8649057 Calculemos ahora el Error Cuadrático Medio \\(ECM=\\frac{1}{n}\\sum(y_i-\\hat{y}_i)^2\\). mean((datos$Resistencia - pred_rls)^2) ## [1] 8312.743 mean((datos$Resistencia - pred_arb)^2) ## [1] 21335.85 ¿Cuál método prefiere usted? 2.3 Estudio de simulación para comparar ambos métodos El objetivo es comparar ambos modelos repetidas veces, para esto vamos a simular conjuntos de datos que tengan un comportamiento lineal y parecido a los datos del ejemplo. El modelo que vamos a considerar es el siguiente: \\[\\begin{align} y_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= 2627 - 37 x_i, \\\\ \\sigma &amp;= 96, \\\\ x &amp;\\sim U(2, 25) \\end{align}\\] Vamos a crear una función generadora de datos. gen_dat &lt;- function(n) { x &lt;- runif(n=n, min=2, max=25) media &lt;- 2627 - 37 * x y &lt;- rnorm(n=n, mean=media, sd=96) data.frame(x=x, y=y) } Generemos unos datos de prueba y graficamos los datos. datos_train &lt;- gen_dat(n=20) with(datos_train, plot(x=x, y=y, pch=20)) Usando los datos de prueba vamos a ajustar los modelos y luego calcularemos los indicadores. datos_train &lt;- gen_dat(n=20) # Para entrenar datos_test &lt;- gen_dat(n=20) # Para validar rls &lt;- lm(y ~ x, data=datos_train) arb &lt;- rpart(y ~ x, data=datos_train) pred_rls &lt;- predict(object=rls, newdata=datos_test) pred_arb &lt;- predict(object=arb, newdata=datos_test) cor(datos_test$y, pred_rls) ## [1] 0.967399 cor(datos_test$y, pred_arb) ## [1] 0.8385178 mean((datos_test$y - pred_rls)^2) ## [1] 6547.405 mean((datos_test$y - pred_arb)^2) ## [1] 25874.69 Ahhhhhh, ¿que está sucediendo? Ahora el modelo de regresión lineal simple se comporta mejor, ¿cómo se explica eso? Ahora vamos a realizar el estudio de simulación, vamos a explorar el efecto de \\(n = 10, 20, 40\\) sobre el \\(ECM\\) usando 5 réplicas para cada \\(n\\), un estudio de simulación “naive” pero ilustrativo. n &lt;- c(10, 20, 40) nrep &lt;- 5 result &lt;- numeric() for (i in n) { for(k in 1:nrep) { datos_train &lt;- gen_dat(n=i) # Para entrenar datos_test &lt;- gen_dat(n=i) # Para validar rls &lt;- lm(y ~ x, data=datos_train) arb &lt;- rpart(y ~ x, data=datos_train) pred_rls &lt;- predict(object=rls, newdata=datos_test) pred_arb &lt;- predict(object=arb, newdata=datos_test) ecm1 &lt;- mean((datos_test$y - pred_rls)^2) ecm2 &lt;- mean((datos_test$y - pred_arb)^2) result &lt;- rbind(result, c(i, ecm1, ecm2)) # No eficiente pero sirve } } colnames(result) &lt;- c(&quot;n&quot;, &quot;ecm_lrs&quot;, &quot;ecm_arb&quot;) result &lt;- as.data.frame(result) result ## n ecm_lrs ecm_arb ## 1 10 11594.584 90497.59 ## 2 10 13425.648 48213.50 ## 3 10 2790.279 80800.36 ## 4 10 9753.380 109843.05 ## 5 10 6924.884 56931.09 ## 6 20 14574.158 25393.17 ## 7 20 19259.531 48612.97 ## 8 20 5306.675 31172.67 ## 9 20 15493.866 51318.35 ## 10 20 16057.186 44374.93 ## 11 40 10719.473 15029.70 ## 12 40 11262.996 17295.56 ## 13 40 8405.420 16125.41 ## 14 40 8501.682 19600.15 ## 15 40 9162.622 22566.15 El objeto result tiene los resultados de la simulación, vamos a calcular el \\(ECM\\) promedio para rls y árboles diferenciando por \\(n\\). library(dplyr) result %&gt;% group_by(n) %&gt;% summarise(ecm_lrs=mean(ecm_lrs), ecm_arb=mean(ecm_arb)) ## # A tibble: 3 x 3 ## n ecm_lrs ecm_arb ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 8898. 77257. ## 2 20 14138. 40174. ## 3 40 9610. 18123. 2.4 Retos A continuación los retos que usted debe aceptar. Extienda el estudio de simulación para otros valores de \\(n\\). Con el estudio de simulación extendido haga un gráfico de \\(ECM\\) promedio versus \\(n\\) para rls y árboles en la misma figura. ¿Cuál \\(ECM\\) es menor? Mirando la figura anterior, ¿se consigue un \\(ECM\\) de cero para un \\(n\\) suficientemente grande? ¿Para qué sirve el paquete dplyr? ¿Qué es un tibble? "],
["references.html", "References", " References "]
]
