[
["index.html", "Modelos Predictivos Bienvenido Estructura del libro Software y convenciones Bloques informativos", " Modelos Predictivos Freddy Hernández Olga Usuga Carmen Patiño 2019-08-07 Bienvenido Este libro está destinado para estudiantes de ingeniería y estadística que deseen aprender sobre modelos de regresión y la forma de aplicarlos por medio del lenguaje de programación R. Freddy Hernández Olga Usuga Carmen Patiño Estructura del libro En el capítulo 1 se presentan los árboles de regresión. Software y convenciones Para realizar este libro usamos los paquetes knitr (Xie 2015) y bookdown (Xie 2019) que permiten unir la ventajas de LaTeX y R en un mismo archivo. En todo el libro se presentarán códigos que el lector puede copiar y pegar en su consola de R para obtener los mismos resultados aquí del libro. Los códigos se destacan en una caja de color similar a la mostrada a continuación. 4 + 6 a &lt;- c(1, 5, 6) 5 * a 1:10 Los resultados o salidas obtenidos de cualquier código se destacan con dos símbolos de númeral (##) al inicio de cada línea o renglón, esto quiere decir que todo lo que inicie con ## son resultados obtenidos y NO los debe copiar. Abajo se muestran los resultados obtenidos luego de correr el código anterior. ## [1] 10 ## [1] 5 25 30 ## [1] 1 2 3 4 5 6 7 8 9 10 Bloques informativos En varias partes del libro usaremos bloques informativos para resaltar algún aspecto importante. Abajo se encuentra un ejemplo de los bloques y su significado. Nota aclaratoria. Sugerencia. Advertencia. References "],
["arb-de-regre.html", "1 Árboles de regresión Paquetes Ejemplo con el paquete rpart Ejemplo con el paquete tree", " 1 Árboles de regresión Los árboles de regresión/clasificación fueron propuestos par Leo Breiman en el libro (Breiman et al. 1984) y son árboles de decisión que tienen como objetivo asignar un valor de \\(\\hat{y}\\) dependiendo de los valores de las covariables. Los árboles se pueden clasificar en dos tipos que son: Árboles de regresión en los cuales la variable respuesta \\(y\\) es cuantitativa. Árboles de clasificación en los cuales la variable respuesta \\(y\\) es cualitativa. El presente capítulo está destinado a árboles de regresión, los árboles de clasificación se explican en el capítulo 2. Un árbol de regresión consiste en hacer preguntas de tipo ¿\\(x_k \\leq c\\)? para cada una de las covariables, de esta forma el espacio de las covariables es divido en hiper-rectángulos y todas las observaciones que queden dentro de un hiper-rectángulo tendrán el mismo valor estimado \\(\\hat{y}\\). En la siguiente figura se ilustra el árbol en el lado izquierdo y la partición del espacio en el lado derecho. La partición del espacio se hace de manera repetitiva para encontrar las variables y los valores de corte \\(c\\) de tal manera que se minimice la función de costos \\(\\sum_{i=1}^{i=n} (y_i - \\hat{y}_i)^2\\). Figure 1.1: Ilustración de la técnica Árboles de Regresión. A la izquierda el árbol y a la derecha la partición del espacio. Los pasos para realizar la partición del espacio son: Dado un conjunto de covariables (características), encontrar la covariable que permita predecir mejor la variable respuesta. Encontrar el punto de corte \\(c\\) sobre esa covariable que permita predecir mejor la variable respuesta. Repetir los pasos anteriores hasta que se alcance el criterio de parada. Algunas de las ventajas de los árboles de regresión son: Fácil de entender e intrepretar. Requiere poca preparación de los datos. Las covariables pueden ser cualitativas o cuantitativas. No exige supuestos distribucionales. Para conocer más sobre el tema se recomienda ver este video y también leer este documento. Paquetes En esta sección se mencionan algunos de los paquetes más comunes para implementar árboles de regresión. El paquete rpart (Therneau and Atkinson 2019) es uno de los paquetes que se pueden usar para crear árboles de regresión. La función para crear un árbol de regresión es rpart, a continuación la estructura de la función. rpart(formula, data, weights, subset, na.action = na.rpart, method, model = FALSE, x = FALSE, y = TRUE, parms, control, cost, ...) Otro paquete útil para árboles de regresión es tree (Ripley 2019). La función para crear un árbol de regresión es tree, a continuación la estructura de la función. tree(formula, data, weights, subset, na.action = na.pass, control = tree.control(nobs, ...), method = &quot;recursive.partition&quot;, split = c(&quot;deviance&quot;, &quot;gini&quot;), model = FALSE, x = FALSE, y = TRUE, wts = TRUE, ...) Se recomienda al lector que consulte la ayuda de las funciones anteriores para que pueda entender las posibilidades que se tienen con cada una de ellas. Ejemplo con el paquete rpart En este ejemplo se busca encontrar un modelo de regresion lineal que explique la variable respuesta \\(y\\) en función de las covariables \\(x_1\\) a \\(x_{11}\\), los datos provienen del ejercicio 9.5 del libro de Montgomery, Peck and Vining (2003). El paquete MPV (Braun 2019) contiene todos los datos que acompañan al libro. A continuación se muestra el encabezado de la base de datos y la definición de las variables. Figure 1.2: Ilustración de los métodos Forward y Backward. Nota: Type of transmission (1=automatic, 0=manual). Antes de iniciar es necesario revisar si hay NA's y eliminarlos. library(MPV) # Aqui estan los datos table.b3[22:26, ] # Can you see the missing values? ## y x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 ## 22 21.47 360.0 180 290 8.4 2.45 2 3 214.2 76.3 4250 1 ## 23 16.59 400.0 185 NA 7.6 3.08 4 3 196.0 73.0 3850 1 ## 24 31.90 96.9 75 83 9.0 4.30 2 5 165.2 61.8 2275 0 ## 25 29.40 140.0 86 NA 8.0 2.92 2 4 176.4 65.4 2150 0 ## 26 13.27 460.0 223 366 8.0 3.00 4 3 228.0 79.8 5430 1 datos &lt;- table.b3[-c(23, 25), ] El objeto datos tiene la base de datos sin las líneas con NA, lo mismo se hubiese podido realizar usando la función na.omit. La base de datos tiene 30 filas y 12 columnas. library(rpart) library(rpart.plot) mod1 &lt;- rpart(y ~ ., data=datos) Dibjuemos el árbol con prp que es una función del paquete rpart.plot (Milborrow 2019). prp(mod1) Construyamos nuevamente el árbol pero explorando todas las opciones de la función prp. prp(mod1, main=&quot;&quot;, nn = TRUE, # display the node numbers fallen.leaves = TRUE, # put the leaves on the bottom of the page shadow.col = &quot;gray&quot;, # shadows under the leaves branch.lty = 3, # draw branches using dotted lines branch = .5, # change angle of branch lines faclen = 0, # faclen = 0 to print full factor names trace = 1, # print the auto calculated cex, xlim, ylim split.cex = 1.2, # make the split text larger than the node text split.prefix = &quot;is &quot;, # put &quot;is &quot; before split text split.suffix = &quot;?&quot;, # put &quot;?&quot; after split text split.box.col = &quot;lightblue&quot;, # lightgray split boxes (default is white) split.border.col = &quot;darkgray&quot;, # darkgray border on split boxes split.round = 0.5) # round the split box corners a tad ## cex 1 xlim c(0, 1) ylim c(0, 1) Usando la información del árbol anterior es posible predecir el valor de \\(y\\). Por ejemplo: Si una nueva observación tiene \\(x_9=70\\) y \\(x_2=100\\), entonces \\(\\hat{y}=20\\). Si otra observación tiene \\(x_9=60\\) y \\(x_2=150\\), entonces \\(\\hat{y}=28\\). Como en el árbol anterior solo aparecen las variables \\(x_2\\) y \\(x_9\\) se recomienda volver a construir el árbol sólo con ellas. mod1 &lt;- rpart(y ~ x2 + x9, data=datos) Este árbol por tener solo dos covariables se puede representar de la siguiente forma: with(datos, plot(x=x2, y=x9)) abline(h=66, lty=&#39;dashed&#39;, col=&#39;blue&#39;) segments(x0=144, y0=66, x1=144, y1=82, lty=&#39;dashed&#39;, col=&#39;blue&#39;) text(x=120, y=63, labels=&#39;y=28&#39;, col=4) text(x=90, y=73, labels=&#39;y=20&#39;, col=4) text(x=190, y=73, labels=&#39;y=16&#39;, col=4) Para predecir los valores de \\(y\\) se puede usar la función predict. A continuación el código para predecir la respuesta en los dos casos anteriores. nuevos_datos &lt;- data.frame(x2=c(100, 150), x9=c(70, 60)) predict(object=mod1, newdata=nuevos_datos) ## 1 2 ## 19.66875 28.06625 En este ejemplo los datos originales se usaron como conjunto de entrenamiento y prueba debido a que solo se cuentan con 30 observaciones. Entre más cerca estén las \\(\\hat{y}\\) de los \\(y\\) observados se puede decir que el modelo es mejor. A continuación la correlación entre \\(\\hat{y}\\) y \\(y\\). y_hat &lt;- predict(object=mod1, newdata=datos) cor(y_hat, datos$y) ## [1] 0.8300304 ¿Qué opina de este valor? A continuación un diagrama de dispersión entre \\(\\hat{y}\\) y \\(y\\). plot(x=datos$y, y=y_hat, pch=20, las=1, xlab=&#39;y&#39;, ylab=expression(hat(y))) abline(a=0, b=1, lty=&quot;dashed&quot;, col=&quot;blue&quot;) Ejemplo con el paquete tree Aquí vamos a repetir el ejemplo anterior con otro paquete. library(tree) mod2 &lt;- tree(y ~ ., data=datos) Para dibujar el árbol se puede usar las siguientes instrucciones. plot(mod2) text(mod2, pretty=0) Entre más cerca estén las \\(\\hat{y}\\) de los \\(y\\) observados se puede decir que el modelo es mejor. A continuación la correlación entre \\(\\hat{y}\\) y \\(y\\). y_hat &lt;- predict(object=mod2, newdata=datos) cor(y_hat, datos$y) ## [1] 0.9265051 References "],
["arb-de-clasif.html", "2 Árboles de clasificación", " 2 Árboles de clasificación Los árboles de regresión/clasificación fueron propuestos par Leo Breiman en el libro (Breiman et al. 1984) y son árboles de decisión que tienen como objetivo asignar un valor de \\(\\hat{y}\\) dependiendo de los valores de las covariables. Los árboles se pueden clasificar en dos tipos que son: Árboles de regresión en los cuales la variable respuesta \\(y\\) es cuantitativa. Árboles de clasificación en los cuales la variable respuesta \\(y\\) es cualitativa. El presente capítulo está destinado a árboles de clasificación, los árboles de clasificación se explican en el capítulo 1. References "],
["reg-versus-arb.html", "3 Regresión lineal versus árboles de regresión Regresión lineal Arboles de regresión Ejemplo Estudio de simulación para comparar ambos métodos Retos", " 3 Regresión lineal versus árboles de regresión En este capítulo se muestra una comparación entre modelos de regresión y árboles de regresion. Regresión lineal El modelo de regresión lineal simple es uno de los más populares en modelación. Este modelo se puede resumir a continuación. \\[\\begin{align} y_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 x_i, \\\\ \\sigma^2 &amp;= \\text{constante} \\end{align}\\] Arboles de regresión Una explicación de los árboles de regresión puede ser consultada en el capítulo 1. Las librerías en R para implementar árboles de regresión son: library(rpart) library(rpart.plot) Ejemplo Como ilustración vamos a usar los datos del ejemplo 2.1 del libro de Montgomery, Peck and Vining (2003). En el ejemplo 2.1 los autores ajustaron un modelo de regresión lineal simple para explicar la Resistencia de una soldadura en función de la Edad de la misma. A continuación el código para cargar los datos y una muestra de las 6 primeras observaciones de la base de datos, en total tenemos 20 observaciones. file &lt;- &quot;https://raw.githubusercontent.com/fhernanb/datos/master/propelente&quot; datos &lt;- read.table(file=file, header=TRUE) head(datos) # shows the first 6 rows ## Resistencia Edad ## 1 2158.70 15.50 ## 2 1678.15 23.75 ## 3 2316.00 8.00 ## 4 2061.30 17.00 ## 5 2207.50 5.50 ## 6 1708.30 19.00 Para crear un diagrama de dispersión que nos muestre la relación entre las dos variables usamos las siguientes instrucciones. library(ggplot2) ggplot(datos, aes(x=Edad, y=Resistencia)) + geom_point() + theme_light() De la figura anterior se ve claramente que a medida que aumenta la edad de la soldadura, la resistencia que ella ofrece disminuye. Adicionalmente, se observa que la relación entre las variables es lineal con una dispersión que parece constante. ¿Quién estima mejor? ¿un modelo de regresión lineal simple o un árbol? rls &lt;- lm(Resistencia ~ Edad, data=datos) arb &lt;- rpart(Resistencia ~ Edad, data=datos) arb &lt;- rpart(Resistencia ~ Edad, data=datos, method=&quot;anova&quot;) ¿Qué hay dentro de modelo de regresión lineal simple? summary(rls) ## ## Call: ## lm(formula = Resistencia ~ Edad, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -215.98 -50.68 28.74 66.61 106.76 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2627.822 44.184 59.48 &lt; 2e-16 *** ## Edad -37.154 2.889 -12.86 1.64e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 96.11 on 18 degrees of freedom ## Multiple R-squared: 0.9018, Adjusted R-squared: 0.8964 ## F-statistic: 165.4 on 1 and 18 DF, p-value: 1.643e-10 ¿Qué hay dentro de modelo del arbol? summary(arb) ## Call: ## rpart(formula = Resistencia ~ Edad, data = datos, method = &quot;anova&quot;) ## n= 20 ## ## CP nsplit rel error xerror xstd ## 1 0.7480619 0 1.0000000 1.127889 0.2409898 ## 2 0.0100000 1 0.2519381 1.127889 0.2409898 ## ## Variable importance ## Edad ## 100 ## ## Node number 1: 20 observations, complexity param=0.7480619 ## mean=2131.358, MSE=84686.88 ## left son=2 (8 obs) right son=3 (12 obs) ## Primary splits: ## Edad &lt; 16.25 to the right, improve=0.7480619, (0 missing) ## ## Node number 2: 8 observations ## mean=1823.094, MSE=19439.95 ## ## Node number 3: 12 observations ## mean=2336.867, MSE=22599.79 Construyamos nuevamente el árbol pero explorando todas las opciones de la función prp. prp(arb, main = &quot;&quot;, nn = TRUE, # display the node numbers fallen.leaves = TRUE, # put the leaves on the bottom of the page shadow.col = &quot;gray&quot;, # shadows under the leaves branch.lty = 3, # draw branches using dotted lines branch = .5, # change angle of branch lines faclen = 0, # faclen = 0 to print full factor names trace = 1, # print the auto calculated cex, xlim, ylim split.cex = 1.2, # make the split text larger than the node text split.prefix = &quot;is &quot;, # put &quot;is &quot; before split text split.suffix = &quot;?&quot;, # put &quot;?&quot; after split text split.box.col = &quot;lightblue&quot;, # lightgray split boxes (default is white) split.border.col = &quot;darkgray&quot;, # darkgray border on split boxes split.round = 0.5) # round the split box corners a tad ## cex 1 xlim c(-0.65, 1.65) ylim c(-0.15, 1.15) A continuación las predicciones con ambos modelos. pred_rls &lt;- predict(object=rls, newdata=datos) pred_arb &lt;- predict(object=arb, newdata=datos) Dibujemos \\(y_i\\) versus \\(\\hat{y}_i\\). par(mfrow=c(1, 2)) plot(x=pred_rls, y=datos$Resistencia, main=&quot;RLS&quot;) abline(a=0, b=1, lty=&quot;dashed&quot;, col=&quot;blue&quot;) plot(x=pred_arb, y=datos$Resistencia, main=&quot;Arbol&quot;) abline(a=0, b=1, lty=&quot;dashed&quot;, col=&quot;blue&quot;) Vamos a calcular \\(Cor(y_i, \\hat{y}_i)\\). cor(datos$Resistencia, pred_rls) ## [1] 0.9496533 cor(datos$Resistencia, pred_arb) ## [1] 0.8649057 Calculemos ahora el Error Cuadrático Medio \\(ECM=\\frac{1}{n}\\sum(y_i-\\hat{y}_i)^2\\). mean((datos$Resistencia - pred_rls)^2) ## [1] 8312.743 mean((datos$Resistencia - pred_arb)^2) ## [1] 21335.85 ¿Cuál método prefiere usted? Estudio de simulación para comparar ambos métodos El objetivo es comparar ambos modelos repetidas veces, para esto vamos a simular conjuntos de datos que tengan un comportamiento lineal y parecido a los datos del ejemplo. El modelo que vamos a considerar es el siguiente: \\[\\begin{align} y_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= 2627 - 37 x_i, \\\\ \\sigma &amp;= 96, \\\\ x &amp;\\sim U(2, 25) \\end{align}\\] Vamos a crear una función generadora de datos. gen_dat &lt;- function(n) { x &lt;- runif(n=n, min=2, max=25) media &lt;- 2627 - 37 * x y &lt;- rnorm(n=n, mean=media, sd=96) data.frame(x=x, y=y) } Generemos unos datos de prueba y graficamos los datos. datos_train &lt;- gen_dat(n=20) ggplot(datos_train, aes(x=x, y=y)) + geom_point() + theme_light() Usando los datos de prueba vamos a ajustar los modelos y luego calcularemos los indicadores. datos_train &lt;- gen_dat(n=20) # Para entrenar datos_test &lt;- gen_dat(n=20) # Para validar rls &lt;- lm(y ~ x, data=datos_train) arb &lt;- rpart(y ~ x, data=datos_train) pred_rls &lt;- predict(object=rls, newdata=datos_test) pred_arb &lt;- predict(object=arb, newdata=datos_test) cor(datos_test$y, pred_rls) ## [1] 0.9479901 cor(datos_test$y, pred_arb) ## [1] 0.6383714 mean((datos_test$y - pred_rls)^2) ## [1] 4979.169 mean((datos_test$y - pred_arb)^2) ## [1] 29444.17 Al observar los resultados anteriores vemos que el modelo de regresión lineal se comporta mejor que el árbol de regresión, esto se debe a que los datos están siendo generados con un modelo de regresión lineal. Ahora vamos a realizar el estudio de simulación para explorar el efecto de \\(n = 10, 20, 40\\) sobre el \\(ECM\\) usando 5 réplicas para cada \\(n\\), este es un estudio de simulación “naive” pero ilustrativo. n &lt;- c(10, 20, 40) nrep &lt;- 5 result &lt;- numeric() for (i in n) { for(k in 1:nrep) { datos_train &lt;- gen_dat(n=i) # Para entrenar datos_test &lt;- gen_dat(n=i) # Para validar rls &lt;- lm(y ~ x, data=datos_train) arb &lt;- rpart(y ~ x, data=datos_train) pred_rls &lt;- predict(object=rls, newdata=datos_test) pred_arb &lt;- predict(object=arb, newdata=datos_test) ecm1 &lt;- mean((datos_test$y - pred_rls)^2) ecm2 &lt;- mean((datos_test$y - pred_arb)^2) result &lt;- rbind(result, c(i, ecm1, ecm2)) # No eficiente pero sirve } } colnames(result) &lt;- c(&quot;n&quot;, &quot;ecm_lrs&quot;, &quot;ecm_arb&quot;) result &lt;- as.data.frame(result) result ## n ecm_lrs ecm_arb ## 1 10 5789.627 33172.16 ## 2 10 11644.021 102638.87 ## 3 10 11005.795 118239.90 ## 4 10 30085.989 75710.61 ## 5 10 6603.893 71380.30 ## 6 20 8233.791 29712.30 ## 7 20 13328.244 32822.36 ## 8 20 8082.185 25235.37 ## 9 20 7115.584 30606.50 ## 10 20 4972.263 23141.04 ## 11 40 12022.350 36495.66 ## 12 40 8134.571 17634.71 ## 13 40 8838.739 15683.04 ## 14 40 7791.612 20358.22 ## 15 40 9651.638 14418.46 El objeto result tiene los resultados de la simulación, vamos a calcular el \\(ECM\\) promedio para rls y árboles diferenciando por \\(n\\). library(dplyr) result %&gt;% group_by(n) %&gt;% summarise(ecm_medio_lrs=mean(ecm_lrs), ecm_medio_arb=mean(ecm_arb)) ## # A tibble: 3 x 3 ## n ecm_medio_lrs ecm_medio_arb ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 13026. 80228. ## 2 20 8346. 28304. ## 3 40 9288. 20918. Retos A continuación los retos que usted debe aceptar. Extienda el estudio de simulación para otros valores de \\(n\\) y aumentando el número de repeticiones nrep, decida usted los valores. Con los resultados anteriores haga un gráfico de \\(ECM\\) promedio versus \\(n\\) para rls y árboles en la misma figura. ¿Se iguala \\(ECM\\) promedio del árbol con el de regresión para algún valor de \\(n\\)? ¿Cuál técnica presenta el \\(ECM\\) menor? ¿Es posible encontrar un \\(ECM=0\\) para algún valor de \\(n\\)? ¿Para qué sirve el paquete dplyr? ¿Qué es un tibble? "],
["references.html", "References", " References "]
]
