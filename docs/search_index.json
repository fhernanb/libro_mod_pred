[
["adaboost.html", "4 AdaBoost Explicación sencilla para aplicar AdaBoost Explicación detallada para aplicar AdaBoost Ejemplo Ejemplo", " 4 AdaBoost AdaBoost (adaptive boosting) fue propuesto por Yoav Freund y Robert Schapire. Explicación sencilla para aplicar AdaBoost Entrene un clasificador. Use el clasificador. Identifique los casos que fueron mal clasificados. Construya un nuevo clasificador que clasifique mejor los casos mal clasificados del punto anterior. Repita los pasos 2 a 4 varias veces. Asígnele un peso a cada clasificador y júntelos para obtener un clasificador con mejor desempeño. Explicación detallada para aplicar AdaBoost Inicie con un conjunto de entrenamiento \\((X, Y)\\) con \\(m\\) observaciones denotadas como \\((x_1, y_1), \\ldots, (x_m, y_m)\\) de tal manera que \\(x_i \\in R^p\\). Los valores de \\(y\\) deben ser -1 o 1 para aplicar el método. Inicie con la distribución discreta \\(D_1(i)=1/m\\) que indica el peso de la observación \\(i\\) en la iteración \\(1\\). Para \\(t=1, \\ldots, T\\). Construya un clasificador \\(h_t\\) definido así: \\(h_t : X \\rightarrow \\{-1, 1 \\}\\). Calcule el error asociado \\(\\epsilon_t\\) al clasificador \\(\\epsilon_t= \\sum_{i=1}^m D_t(i) \\times \\delta_i\\), donde \\(\\delta_i=0\\) si \\(h_t(x_i)=y_i\\), es decir, si fue correcta la clasificación; caso contrario es \\(\\delta_i=1\\). Calcule la nueva distribución \\(D_{t+1}(i)=D_{t}(i) \\times F_i / Z_t\\), donde: \\(F_i=\\exp(-\\alpha_t)\\) si la clasificación fue correcta, es decir si \\(h_t(x_i) = y_i\\). \\(F_i=\\exp(\\alpha_t)\\) si la clasificación fue incorrecta, es decir si \\(h_t(x_i) \\neq y_i\\). \\(\\alpha_t=\\frac{1}{2} \\log \\left( \\frac{1-\\epsilon_t}{\\epsilon_t} \\right)\\). \\(Z_t\\) es una constante de normalización de tal manera que \\(\\sum_{i=1}^m D_t(i)=1\\). Usualmente es \\(\\sum D_{t}(i) \\times F_i\\). Construya el clasificador final \\(H_{final}\\) como el promedio ponderado de los \\(t\\) clasificadores \\(h_t\\), usando \\(H_{final}=sign(\\sum_t \\alpha_t h_t(x))\\). Ejemplo En este ejemplo se ilustra la forma de Figure 4.1: Datos originales. Figure 4.2: Clasificadores. Figure 4.3: Obtención clasificador final. Figure 4.4: Clasificador final. Ejemplo La base de datos que vamos a usar en este ejemplo está disponible en el UCI Repository. La variable respuesta es Target codificada como 0 = si el paciente no sufrió un ataque cardiaco y como 1, 2, 3 o 4 si el paciente sufrió un ataque cardiaco. La variable se re-codificará con los valores de -1 y 1 así: \\[ Y=\\left\\{\\begin{matrix} 1 \\quad \\text{si paciente SI sufre una enfermedad cardíaca} \\\\ -1 \\quad \\text{si paciente NO sufre una enfermedad cardíaca} \\end{matrix}\\right. \\] El objetivo es crear un árbol de clasificación para predecir la variable \\(Y\\) (target). Primero vamos a cargar los datos. library(readr) url &lt;- &#39;https://raw.githubusercontent.com/fhernanb/datos/master/cleveland.csv&#39; datos &lt;- read_csv(url, col_names = FALSE) ## Parsed with column specification: ## cols( ## X1 = col_double(), ## X2 = col_double(), ## X3 = col_double(), ## X4 = col_double(), ## X5 = col_double(), ## X6 = col_double(), ## X7 = col_double(), ## X8 = col_double(), ## X9 = col_double(), ## X10 = col_double(), ## X11 = col_double(), ## X12 = col_double(), ## X13 = col_double(), ## X14 = col_double() ## ) colnames(datos) &lt;- c(&#39;age&#39;, &#39;sex&#39;, &#39;cp&#39;, &#39;trestbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalach&#39;, &#39;exang&#39;, &#39;oldpeak&#39;, &#39;slope&#39;, &#39;ca&#39;, &#39;thal&#39;, &#39;target&#39;) datos$yy &lt;- ifelse(datos$target == 0, -1, 1) Ahora vamos a aplicar el algoritmo descrito arriba con \\(T=3\\). library(rpart) m &lt;- nrow(datos) Dt &lt;- rep(1/m, m) # D1 # Primer clasificador mod1 &lt;- rpart(yy ~ age + chol, weights=Dt, data=datos, method=&#39;class&#39;) y_hat &lt;- predict(mod1, type=&#39;class&#39;) error &lt;- ifelse(y_hat == datos$yy, 0, 1) epsilon_t &lt;- sum(error * Dt) alpha_t &lt;- 0.5 * log((1-epsilon_t)/epsilon_t) alpha1 &lt;- alpha_t Fi &lt;- ifelse(y_hat == datos$yy, exp(-alpha_t), exp(alpha_t)) Dt &lt;- Dt * Fi Dt &lt;- Dt / sum(Dt) # Segundo clasificador mod2 &lt;- rpart(yy ~ age + chol, weights=Dt, data=datos, method=&#39;class&#39;) y_hat &lt;- predict(mod2, type=&#39;class&#39;) error &lt;- ifelse(y_hat == datos$yy, 0, 1) epsilon_t &lt;- sum(error * Dt) alpha_t &lt;- 0.5 * log((1-epsilon_t)/epsilon_t) alpha2 &lt;- alpha_t Fi &lt;- ifelse(y_hat == datos$yy, exp(-alpha_t), exp(alpha_t)) Dt &lt;- Dt * Fi Dt &lt;- Dt / sum(Dt) # Tercer clasificador mod3 &lt;- rpart(yy ~ age + chol, weights=Dt, data=datos, method=&#39;class&#39;) y_hat &lt;- predict(mod3, type=&#39;class&#39;) error &lt;- ifelse(y_hat == datos$yy, 0, 1) epsilon_t &lt;- sum(error * Dt) alpha_t &lt;- 0.5 * log((1-epsilon_t)/epsilon_t) alpha3 &lt;- alpha_t Fi &lt;- ifelse(y_hat == datos$yy, exp(-alpha_t), exp(alpha_t)) Dt &lt;- Dt * Fi Dt &lt;- Dt / sum(Dt) El clasificador final está definido como \\(H_{final}=sign(\\sum_t \\alpha_t h_t(x))\\) y para obtenerlo en R usamos las siguientes instrucciones. y_hat1 &lt;- ifelse(predict(mod1, type=&#39;class&#39;) == &#39;-1&#39;, -1, 1) y_hat2 &lt;- ifelse(predict(mod1, type=&#39;class&#39;) == &#39;-1&#39;, -1, 1) y_hat3 &lt;- ifelse(predict(mod1, type=&#39;class&#39;) == &#39;-1&#39;, -1, 1) y_final &lt;- sign(alpha1 * y_hat1 + alpha2 * y_hat2 + alpha3 * y_hat3) Para ver las primeras clasificaciones usarmos head(y_final) ## [1] 1 -1 -1 -1 -1 1 "]
]
