# Árboles de regresión {#arb-de-regre}
Los árboles de regresión son árboles de decisión para asignar un valor de $\hat{y}$ dependiendo de los valores de las covariables. Para conocer más sobre el tema se recomienda ver [este video](https://www.youtube.com/watch?v=7VeUPuFGJHk).

El paquete **rpart** [@R-rpart] es uno de los paquetes que se pueden usar para crear árboles de regresión. La función para crear un árbol de regresión es `rpart`, a continuación la estructura de la función.

```{r, eval=FALSE}
rpart(formula, data, weights, subset, na.action = na.rpart, method,
      model = FALSE, x = FALSE, y = TRUE, parms, control, cost, ...)
```

## Ejemplo {-}
En este ejemplo se busca encontrar un modelo de regresion lineal que explique la variable respuesta $y$ en función de las covariables $x_1$ a $x_{11}$, los datos provienen del ejercicio 9.5 del libro de [Montgomery, Peck and Vining (2003)](https://www.amazon.com/Introduccion-analisis-regresion-lineal-Spanish/dp/9702403278). El paquete **MPV** [@R-MPV] contiene todos los datos que acompañan al libro.

A continuación se muestra el encabezado de la base de datos y la definición de las variables.

```{r selection02, echo=F, fig.cap='Ilustración de los métodos Forward y Backward.', dpi=60, fig.align='center'}
knitr::include_graphics("images/tableb3.png")
```

__Nota__: Type of transmission (1=automatic, 0=manual).

Antes de iniciar es necesario revisar si hay `NA's` y eliminarlos.

```{r, message=F}
library(MPV)  # Aqui estan los datos
table.b3[22:26, ] # Can you see the missing values?
datos <- table.b3[-c(23, 25), ]
```

El objeto `datos` tiene la base de datos sin las líneas con `NA`, lo mismo se hubiese podido realizar usando la función `na.omit`. La base de datos tiene `r nrow(datos)` filas y `r ncol(datos)` columnas.

```{r, message=FALSE}
library(rpart)
library(rpart.plot)
```

```{r}
mod <- rpart(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11,
             data=datos)
```

Dibjuemos el árbol con `prp` que es una función del paquete **rpart.plot** [@R-rpart.plot].

```{r arbol01, fig.align='center', fig.height=3, fig.width=6}
prp(mod)
```

Construyamos nuevamente el árbol pero explorando todas las opciones de la función `prp`.

```{r arbol02, fig.align='center', fig.height=3, fig.width=6}
prp(mod, main="",
    nn = TRUE,             # display the node numbers
    fallen.leaves = TRUE,  # put the leaves on the bottom of the page
    shadow.col = "gray",   # shadows under the leaves
    branch.lty = 3,        # draw branches using dotted lines
    branch = .5,           # change angle of branch lines
    faclen = 0,            # faclen = 0 to print full factor names
    trace = 1,             # print the auto calculated cex, xlim, ylim
    split.cex = 1.2,       # make the split text larger than the node text
    split.prefix = "is ",  # put "is " before split text
    split.suffix = "?",    # put "?" after split text
    split.box.col = "lightblue",   # lightgray split boxes (default is white)
    split.border.col = "darkgray", # darkgray border on split boxes
    split.round = 0.5)             # round the split box corners a tad
```

Usando la información del árbol anterior es posible predecir el valor de $y$. Por ejemplo:

1. Si una nueva observación tiene $x_9=70$ y $x_2=100$, entonces $\hat{y}=20$.
2. Si otra observación tiene $x_9=60$ y $x_2=150$, entonces $\hat{y}=28$.

Como en el árbol anterior solo aparecen las variables $x_2$ y $x_9$ se recomienda volver a construir el árbol sólo con ellas.

```{r}
mod <- rpart(y ~ x2 + x9, data=datos)
```

Para predecir los valores de $y$ se puede usar la función `predict`. A continuación el código para predecir la respuesta en los dos casos anteriores.

```{r}
nuevos_datos <- data.frame(x2=c(100, 150), x9=c(70, 60))
predict(object=mod, newdata=nuevos_datos)
```

En este ejemplo los datos originales se usaron como conjunto de entrenamiento y prueba debido a que solo se cuentan con `r nrow(datos)` observaciones.

Entre más cerca estén las $\hat{y}$ de los $y$ observados se puede decir que el modelo es mejor. A continuación la correlación entre $\hat{y}$ y $y$.

```{r}
y_hat <- predict(object=mod, newdata=datos)
cor(y_hat, datos$y)
```

¿Qué opina de este valor?

A continuación un diagrama de dispersión entre $\hat{y}$ y $y$.

```{r arbol03, fig.align='center', fig.height=4, fig.width=4}
plot(x=datos$y, y=y_hat, pch=20, las=1, xlab='y', ylab=expression(hat(y)))
abline(a=0, b=1, lty="dashed", col="blue")
```


