[
["index.html", "Modelos Predictivos Bienvenido Estructura del libro Software y convenciones Bloques informativos", " Modelos Predictivos Freddy Hernández 2020-10-25 Bienvenido Este libro está destinado para quienes deseen construir modelos de predección y la forma de aplicarlos por medio del lenguaje de programación R. Freddy Hernández Estructura del libro En el capítulo 1 se presentan los árboles de regresión. Software y convenciones Para realizar este libro usamos los paquetes knitr (Xie 2015) y bookdown (Xie 2020) que permiten unir la ventajas de LaTeX y R en un mismo archivo. En todo el libro se presentarán códigos que el lector puede copiar y pegar en su consola de R para obtener los mismos resultados aquí del libro. Los códigos se destacan en una caja de color similar a la mostrada a continuación. 4 + 6 a &lt;- c(1, 5, 6) 5 * a 1:10 Los resultados o salidas obtenidos de cualquier código se destacan con dos símbolos de númeral (##) al inicio de cada línea o renglón, esto quiere decir que todo lo que inicie con ## son resultados obtenidos y NO los debe copiar. Abajo se muestran los resultados obtenidos luego de correr el código anterior. ## [1] 10 ## [1] 5 25 30 ## [1] 1 2 3 4 5 6 7 8 9 10 Bloques informativos En varias partes del libro usaremos bloques informativos para resaltar algún aspecto importante. Abajo se encuentra un ejemplo de los bloques y su significado. Nota aclaratoria. Sugerencia. Advertencia. References "],
["arb-de-regre.html", "1 Árboles de regresión Árboles Árbol de decisión Tipos de árboles Árbol de regresión Paquetes de R para construir árboles Paquete rpart Paquete tree Ejemplo con el paquete rpart Ejemplo con el paquete tree", " 1 Árboles de regresión Los árboles de regresión/clasificación fueron propuestos por Leo Breiman en el libro (Breiman et al. 1984) y son árboles de decisión que tienen como objetivo predecir la variable respuesta \\(Y\\) en función de covariables. Árboles A continuación la imagen de un árbol común. A continuación la imagen de un árbol seco e invertido. Esta forma invertida es la forma de los árboles de regresión y clasificación. Árbol de decisión Conjunto de reglas sucesivas que ayudan a tomar una decisión. Tipos de árboles Los árboles se pueden clasificar en dos tipos que son: Árboles de regresión en los cuales la variable respuesta \\(y\\) es cuantitativa. Árboles de clasificación en los cuales la variable respuesta \\(y\\) es cualitativa. El presente capítulo está destinado a árboles de clasificación, los árboles de regresión se explican en el capítulo 2. Árbol de regresión Un árbol de regresión consiste en hacer preguntas de tipo ¿\\(x_k \\leq c\\)? para cada una de las covariables, de esta forma el espacio de las covariables es divido en hiper-rectángulos y todas las observaciones que queden dentro de un hiper-rectángulo tendrán el mismo valor estimado \\(\\hat{y}\\). En la siguiente figura se ilustra el árbol en el lado izquierdo y la partición del espacio en el lado derecho. La partición del espacio se hace de manera repetitiva para encontrar las variables y los valores de corte \\(c\\) de tal manera que se minimice la función de costos \\(\\sum_{i=1}^{i=n} (y_i - \\hat{y}_i)^2\\). Los pasos para realizar la partición del espacio son: Dado un conjunto de covariables (características), encontrar la covariable que permita predecir mejor la variable respuesta. Encontrar el punto de corte \\(c\\) sobre esa covariable que permita predecir mejor la variable respuesta. Repetir los pasos anteriores hasta que se alcance el criterio de parada. Algunas de las ventajas de los árboles de regresión son: Fácil de entender e intrepretar. Requiere poca preparación de los datos. Las covariables pueden ser cualitativas o cuantitativas. No exige supuestos distribucionales. Para explicaciones más detalladas sobre las técnicas basadas en árboles recomendamos consultar el capítulo 8 de (James et al. 2013). Se recomienda también ver este video con una explicación sencilla sobre árboles. Paquetes de R para construir árboles Los paquetes más conocidos para construir árboles son: tree. rpart. party Existen otros paquetes relacionados con la construcción de árboles y el lector los puede consultar en la sección Recursive Partitioning de CRAN Task View: Machine Learning &amp; Statistical Learning. Paquete rpart En esta sección se mencionan algunos de los paquetes más comunes para implementar árboles de regresión. El paquete rpart (Therneau and Atkinson 2019) es uno de los paquetes que se pueden usar para crear árboles de regresión. La función para crear un árbol de regresión es rpart, a continuación la estructura de la función. rpart(formula, data, weights, subset, na.action = na.rpart, method, model = FALSE, x = FALSE, y = TRUE, parms, control, cost, ...) Paquete tree Otro paquete útil para árboles de regresión es tree (Ripley 2019). La función para crear un árbol de regresión es tree, a continuación la estructura de la función. tree(formula, data, weights, subset, na.action = na.pass, control = tree.control(nobs, ...), method = &quot;recursive.partition&quot;, split = c(&quot;deviance&quot;, &quot;gini&quot;), model = FALSE, x = FALSE, y = TRUE, wts = TRUE, ...) Se recomienda al lector que consulte la ayuda de las funciones anteriores para que pueda entender las posibilidades que se tienen con cada una de ellas. Ejemplo con el paquete rpart En este ejemplo se busca encontrar un modelo de regresion lineal que explique la variable respuesta \\(y\\) en función de las covariables \\(x_1\\) a \\(x_{11}\\), los datos provienen del ejercicio 9.5 del libro de Montgomery, Peck and Vining (2003). El paquete MPV (Braun 2019) contiene todos los datos que acompañan al libro. A continuación se muestra el encabezado de la base de datos y la definición de las variables. Nota: Type of transmission (1=automatic, 0=manual). Antes de iniciar es necesario revisar si hay NA's y eliminarlos. library(MPV) # Aqui estan los datos table.b3[22:26, ] # Can you see the missing values? ## y x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 ## 22 21.47 360.0 180 290 8.4 2.45 2 3 214.2 76.3 4250 1 ## 23 16.59 400.0 185 NA 7.6 3.08 4 3 196.0 73.0 3850 1 ## 24 31.90 96.9 75 83 9.0 4.30 2 5 165.2 61.8 2275 0 ## 25 29.40 140.0 86 NA 8.0 2.92 2 4 176.4 65.4 2150 0 ## 26 13.27 460.0 223 366 8.0 3.00 4 3 228.0 79.8 5430 1 datos &lt;- table.b3[-c(23, 25), ] El objeto datos tiene la base de datos sin las líneas con NA, lo mismo se hubiese podido realizar usando la función na.omit. La base de datos tiene 30 filas y 12 columnas. library(rpart) library(rpart.plot) mod1 &lt;- rpart(y ~ ., data=datos) Dibjuemos el árbol con prp que es una función del paquete rpart.plot (Milborrow 2019). prp(mod1) Construyamos nuevamente el árbol pero explorando todas las opciones de la función prp. prp(mod1, main=&quot;&quot;, nn = TRUE, # display the node numbers fallen.leaves = TRUE, # put the leaves on the bottom of the page shadow.col = &quot;gray&quot;, # shadows under the leaves branch.lty = 3, # draw branches using dotted lines branch = .5, # change angle of branch lines faclen = 0, # faclen = 0 to print full factor names trace = 1, # print the auto calculated cex, xlim, ylim split.cex = 1.2, # make the split text larger than the node text split.prefix = &quot;is &quot;, # put &quot;is &quot; before split text split.suffix = &quot;?&quot;, # put &quot;?&quot; after split text split.box.col = &quot;lightblue&quot;, # lightgray split boxes (default is white) split.border.col = &quot;darkgray&quot;, # darkgray border on split boxes split.round = 0.5) # round the split box corners a tad ## cex 1 xlim c(0, 1) ylim c(0, 1) Usando la información del árbol anterior es posible predecir el valor de \\(y\\). Por ejemplo: Si una nueva observación tiene \\(x_9=70\\) y \\(x_2=100\\), entonces \\(\\hat{y}=20\\). Si otra observación tiene \\(x_9=60\\) y \\(x_2=150\\), entonces \\(\\hat{y}=28\\). Como en el árbol anterior solo aparecen las variables \\(x_2\\) y \\(x_9\\) se recomienda volver a construir el árbol sólo con ellas. mod1 &lt;- rpart(y ~ x2 + x9, data=datos) Este árbol por tener solo dos covariables se puede representar de la siguiente forma: with(datos, plot(x=x2, y=x9)) abline(h=66, lty=&#39;dashed&#39;, col=&#39;blue&#39;) segments(x0=144, y0=66, x1=144, y1=82, lty=&#39;dashed&#39;, col=&#39;blue&#39;) text(x=120, y=63, labels=&#39;y=28&#39;, col=4) text(x=90, y=73, labels=&#39;y=20&#39;, col=4) text(x=190, y=73, labels=&#39;y=16&#39;, col=4) Para predecir los valores de \\(y\\) se puede usar la función predict. A continuación el código para predecir la respuesta en los dos casos anteriores. nuevos_datos &lt;- data.frame(x2=c(100, 150), x9=c(70, 60)) predict(object=mod1, newdata=nuevos_datos) ## 1 2 ## 19.66875 28.06625 En este ejemplo los datos originales se usaron como conjunto de entrenamiento y prueba debido a que solo se cuentan con 30 observaciones. Entre más cerca estén las \\(\\hat{y}\\) de los \\(y\\) observados se puede decir que el modelo es mejor. A continuación la correlación entre \\(\\hat{y}\\) y \\(y\\). y_hat &lt;- predict(object=mod1, newdata=datos) cor(y_hat, datos$y) ## [1] 0.8300304 ¿Qué opina de este valor? A continuación un diagrama de dispersión entre \\(\\hat{y}\\) y \\(y\\). plot(x=datos$y, y=y_hat, pch=20, las=1, xlab=&#39;y&#39;, ylab=expression(hat(y))) abline(a=0, b=1, lty=&quot;dashed&quot;, col=&quot;blue&quot;) Ejemplo con el paquete tree Aquí vamos a repetir el ejemplo anterior con otro paquete. library(tree) mod2 &lt;- tree(y ~ ., data=datos) Para dibujar el árbol se puede usar las siguientes instrucciones. plot(mod2) text(mod2, pretty=0) Entre más cerca estén las \\(\\hat{y}\\) de los \\(y\\) observados se puede decir que el modelo es mejor. A continuación la correlación entre \\(\\hat{y}\\) y \\(y\\). y_hat &lt;- predict(object=mod2, newdata=datos) cor(y_hat, datos$y) ## [1] 0.9265051 References "],
["arb-de-clasif.html", "2 Árboles de clasificación Árboles Árbol de decisión Tipos de árboles Árbol de clasificación Paquetes de R para construir árboles Ejemplo con rpart Ejemplo con tree Ejemplo", " 2 Árboles de clasificación Los árboles de regresión/clasificación fueron propuestos por Leo Breiman en el libro (Breiman et al. 1984) y son árboles de decisión que tienen como objetivo predecir la variable respuesta \\(Y\\) en función de covariables. Árboles A continuación la imagen de un árbol común. A continuación la imagen de un árbol seco e invertido. Esta forma invertida es la forma de los árboles de regresión y clasificación. Árbol de decisión Conjunto de reglas sucesivas que ayudan a tomar una decisión. Tipos de árboles Los árboles se pueden clasificar en dos tipos que son: Árboles de regresión en los cuales la variable respuesta \\(y\\) es cuantitativa. Árboles de clasificación en los cuales la variable respuesta \\(y\\) es cualitativa. El presente capítulo está destinado a árboles de clasificación, los árboles de regresión se explican en el capítulo 1. Árbol de clasificación Un árbol de regresión consiste en hacer preguntas del tipo ¿\\(x_k \\leq c\\)? para las covariables cuantitativas o preguntas del tipo ¿\\(x_k = \\text{nivel}_j\\)? para las covariables cualitativas, de esta forma el espacio de las covariables es divido en hiper-rectángulos y todas las observaciones que queden dentro de un hiper-rectángulo tendrán el mismo valor grupo estimado. En la siguiente figura se ilustra el árbol en el lado izquierdo y la partición del espacio en el lado derecho. La partición del espacio se hace de manera repetitiva para encontrar las variables y los valores de corte \\(c\\) de tal manera que se minimice la función de costos \\(\\sum_{i=1}^{i=n} (y_i - \\hat{y}_i)^2\\). Paquetes de R para construir árboles Los paquetes más conocidos para construir árboles son: tree. rpart. party Existen otros paquetes relacionados con la construcción de árboles y el lector los puede consultar en la sección Recursive Partitioning de CRAN Task View: Machine Learning &amp; Statistical Learning. Ejemplo con rpart La base de datos que vamos a usar en este ejemplo está disponible en el UCI Repository. El objetivo es crear un árbol de clasificación para predecir la variable \\(Y\\) (target) definida como: \\[ Y=\\left\\{\\begin{matrix} 1 \\quad \\text{si paciente SI sufre una enfermedad cardíaca} \\\\ 0 \\quad \\text{si paciente NO sufre una enfermedad cardíaca} \\end{matrix}\\right. \\] en función de las variables age y chol. Las variables en la base de datos son las siguientes: Age: displays the age of the individual. Sex: displays the gender of the individual using the following format: 1 = male, 0 = female. Chest-pain type: displays the type of chest-pain experienced by the individual using the following format: 1 = typical angina, 2 = atypical angina, 3 = non — anginal pain, 4 = asymptotic. Resting Blood Pressure: displays the resting blood pressure value of an individual in mmHg (unit) Chol: displays the serum cholesterol in mg/dl (unit) Fasting Blood Sugar: compares the fasting blood sugar value of an individual with 120mg/dl. If fasting blood sugar &gt; 120mg/dl then : 1 (true) else : 0 (false). Resting ECG : displays resting electrocardiographic results: 0 = normal 1 = having ST-T wave abnormality, 2 = left ventricular hyperthrophy. Max heart rate achieved: displays the max heart rate achieved by an individual. Exercise induced angina: 1 = yes, 0 = no. ST depression induced by exercise relative to rest: displays the value which is an integer or float. Peak exercise ST segment: 1 = upsloping, 2 = flat, 3 = downsloping. Number of major vessels (0–3) colored by flourosopy : displays the value as integer or float. Thal: displays the thalassemia: 3 = normal, 6 = fixed defect, 7 = reversible defect. Target: Diagnosis of heart disease. Displays whether the individual is suffering from heart disease or not: 0 = absence, 1, 2, 3, 4 = present. La base de datos está en un repositorio en la web y se puede leer usando el siguiente código. library(readr) url &lt;- &#39;https://raw.githubusercontent.com/fhernanb/datos/master/cleveland.csv&#39; datos &lt;- read_csv(url, col_names = FALSE) Como la base de datos viene sin los nombres se deben colocar manualmente así: colnames(datos) &lt;- c(&#39;age&#39;, &#39;sex&#39;, &#39;cp&#39;, &#39;trestbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalach&#39;, &#39;exang&#39;, &#39;oldpeak&#39;, &#39;slope&#39;, &#39;ca&#39;, &#39;thal&#39;, &#39;target&#39;) La variable respuesta es target que tiene cuatro números así: 0 = absence, 1, 2, 3, 4 = present. Por esa razón vamos a crear la nueva variable y que agregaremos a la base de datos usando el siguiente código. datos$y &lt;- ifelse(datos$target == 0, &#39;absence&#39;, &#39;presence&#39;) datos$y &lt;- as.factor(datos$y) Nota: en orden lexicográfico la etiqueta absence está primero que presence porque inicia con la letra a. Eso significa que el árbol va a tomar la etiqueta absence como 0 mientras que presence la va a tomar como 1. Nota: se pudo haber creado la variable y usando ifelse(datos$target == 0, 0, 1) pero decidimos usar los nombres absence y presence en lugar del 0 y 1 para facilitar su interpretación. ¿Cuántos pacientes hay en la base de datos? nrow(datos) ## [1] 303 ¿Cuántos pacientes presentan la enfermedad y cuántos no? table(datos$y) ## ## absence presence ## 164 139 Ahora vamos a crear un diagrama de dispersión de chol versus age diferenciando por y para observar si hay algún patrón claro a nuestros ojos. library(ggplot2) ggplot(datos, aes(x=age, y=chol, col=y)) + geom_point() + labs(x=&#39;Edad&#39;, y=&#39;Nivel colesterol&#39;) Para crear el árbol de clasificación usaremos la función rpart del paquete rpart. Para obtener más detalles de este paquete se recomienda consultar las viñetas del paquete disponibles en este enlace. El código para crear el árbol es el siguiente. library(rpart) mod1 &lt;- rpart(y ~ age + chol, data=datos, method=&#39;class&#39;) Para dibujar el árbol de clasificación podemos usar la función rpart.plot del paquete rpart.plot. Para obtener más detalles de este paquete se recomienda consultar la viñeta del paquete disponible en este enlace. library(rpart.plot) rpart.plot(mod1, type=0, extra=0, box.palette = c(&quot;lightgreen&quot;, &quot;pink&quot;)) Si nos cuesta trabajo entender el árbol anterior podemos generar las reglas de clasificación en un lenguaje sencillo así: rpart.rules(mod1) ## y ## 0.26 when age &lt; 55 &amp; chol &lt; 274 ## 0.38 when age &gt;= 64 &amp; chol &gt;= 259 ## 0.38 when age &lt; 55 &amp; chol &gt;= 287 ## 0.45 when age &gt;= 64 &amp; chol &lt; 242 ## 0.65 when age is 55 to 64 ## 0.86 when age &lt; 55 &amp; chol is 274 to 287 ## 0.86 when age &gt;= 64 &amp; chol is 242 to 259 La primera línea de la salida anterior nos dice que “P(presence) = 0.26 para pacientes cuyo perfil es menor de 55 años y colesterol menor de 274”. La última línea de la salida anterior nos dice que “P(presence) = 0.86 para pacientes cuyo perfil es mayor o igual a 64 años y colesterol entre 242 y 259”. Para construir la tabla de confusión usando los datos de entrenamiento podemos usar el siguiente código. y_hat &lt;- predict(mod1, type=&#39;class&#39;) tabla &lt;- table(Verdadero=datos$y, Clasificacion=y_hat) tabla ## Clasificacion ## Verdadero absence presence ## absence 124 40 ## presence 56 83 Para obtener la tasa de clasificación correcta podemos usar el siguiente código. sum(diag(tabla)) / sum(tabla) ## [1] 0.6831683 Supongamos que nos llegaron 3 pacientes, el primero un hombre de 45 años con nivel de colesterol 300, el segundo una mujer de 60 años con nivel de colesterol de 450 y el tercero otro hombre de 65 años con nivel de colesterol de 200. ¿A qué grupo se debe asignar cada paciente? nuevo_df &lt;- data.frame(age=c(45, 60, 65), chol=c(300, 450, 200)) predict(mod1, newdata=nuevo_df, type=&#39;prob&#39;) ## absence presence ## 1 0.6190476 0.3809524 ## 2 0.3486239 0.6513761 ## 3 0.5500000 0.4500000 Si queremos obtener las etiquetas de clasificación podemos usar: predict(mod1, newdata=nuevo_df, type=&#39;class&#39;) ## 1 2 3 ## absence presence absence ## Levels: absence presence Supongamos que nos llegan 3 nuevos pacientes: el primero un hombre de 45 años pero no sabemos su nivel de colesterol, el segundo una mujer de edad desconocida y con nivel de colesterol de 450 y el tercero otro hombre del cual no sabemos ni su edad ni su nivel de colesterol. ¿A qué grupo se debe asignar cada paciente? nuevo_df &lt;- data.frame(age=c(45, NA, NA), chol=c(NA, 450, NA)) predict(mod1, newdata=nuevo_df, type=&#39;prob&#39;) ## absence presence ## 1 0.7391304 0.2608696 ## 2 0.6250000 0.3750000 ## 3 0.3486239 0.6513761 predict(mod1, newdata=nuevo_df, type=&#39;class&#39;) ## 1 2 3 ## absence absence presence ## Levels: absence presence Ejemplo con tree Vamos a construir un árbol de clasificación para los mismos datos del ejemplo anterior pero usando la función tree del paquete tree. library(tree) mod2 &lt;- tree(y ~ age + chol, data=datos) Para dibujar el árbol. plot(mod2, type=&#39;uniform&#39;) text(mod2) Supongamos que nos llegaron 3 pacientes, el primero un hombre de 45 años con nivel de colesterol 300, el segundo una mujer de 60 años con nivel de colesterol de 450 y el tercero otro hombre de 65 años con nivel de colesterol de 200. ¿A qué grupo se debe asignar cada paciente? nuevo_df &lt;- data.frame(age=c(45, 60, 65), chol=c(300, 450, 200)) predict(mod2, newdata=nuevo_df, type=&#39;vector&#39;) ## absence presence ## 1 0.5000000 0.5000000 ## 2 0.7000000 0.3000000 ## 3 0.4567901 0.5432099 predict(mod2, newdata=nuevo_df, type=&#39;class&#39;) ## [1] presence absence presence ## Levels: absence presence Ejemplo Comparemos los resultados obtenidos con las funciones rpart y tree. y_rpart &lt;- predict(mod1, type=&#39;class&#39;) y_tree &lt;- predict(mod2, type=&#39;class&#39;) table(datos$y, y_rpart) ## y_rpart ## absence presence ## absence 124 40 ## presence 56 83 table(datos$y, y_tree) ## y_tree ## absence presence ## absence 117 47 ## presence 47 92 References "],
["svm-clas.html", "3 Support Vector Machines para clasificación", " 3 Support Vector Machines para clasificación Cortes and Vapnik (1995) propusieron las máquinas de soporte vectorial para el problema de clasificación. Suponga que deseamos tenemos dos grupos de objetos, unos de color rojo y otros de color amarillo como se muestran en la siguiente figura. Figure 3.1: Ilustración de la técnica Árboles de Regresión. A la izquierda el árbol y a la derecha la partición del espacio. El objetivo es dibujar una línea recta que separe los dos grupos, sin embargo, muchas líneas se podrían dibujar, a continuación se muestran tres posibles líneas con las cuales se consigue el objetivo. Figure 3.2: espacio2. Imaginemos que cada línea es como una carretera que se puede ampliar a ambos lados hasta que toque el punto más cercano, ya se amarillo o rojo. Al hacer esto vamos a obtener las tres carreteras que se muestran a continuación. Figure 3.3: espacio3. De todas las carreteras nos interesa aquella que tenga el mayor ancho o margen, con esa carretera es que se pueden clasificar nuevas observaciones en el grupo rojo o grupo amarillo. A continuación se muestra la figura sólo con la línea de separación que tiene el mayor ancho. Figure 3.4: espacio4. References "],
["svm-reg.html", "4 Support Vector Machines para regresión", " 4 Support Vector Machines para regresión Drucker et al. (1997) en el artículo titulado “Support Vector Regression Machines” propusieron las máquinas de soporte vectorial para el problema de regresión. En la siguiente figura se ilustra la idea intuitiva para el caso de predecir \\(y\\) usando una sola variable \\(x\\). El objetivo es construir un margen de tolerancia (\\(\\varepsilon\\)) con el cual se tendrán observaciones dentro de los márgenes y fuera de los márgenes. Usando sólo las observaciones fuera de los márgenes, se calculan los errores \\(\\xi\\) o \\(\\xi^\\star\\) y con ellos se construye la función de objetivo \\(FO\\) a minimizar \\[ FO = \\frac{1}{2} \\| \\boldsymbol{w} \\|^2 + C \\sum_{i=1}^{N} (\\xi_i + \\xi^\\star), \\] donde \\(\\boldsymbol{w}\\) es el vector con las pendientes asociadas a cada una de las variables (sin incluir intercepto), \\(C\\) es un valor de penalización para los errores y \\(m\\) el número de observaciones que están fuera de los márgenes del total de \\(n\\) observaciones. References "],
["adaboost.html", "5 AdaBoost Explicación sencilla de AdaBoost Explicación detallada de AdaBoost Ejemplo Ejemplo Ejemplo Ejemplo", " 5 AdaBoost AdaBoost (adaptive boosting) fue propuesto por (Freund and Schapire 1995) y consiste en crear varios predictores sencillos en secuencia, de tal manera que el segundo ajuste bien lo que el primero no ajustó, que el tercero ajuste un poco mejor lo que el segundo no pudo ajustar y así sucesivamente. En la siguiente figura se muestra una ilustración de lo que es AdaBoost. Explicación sencilla de AdaBoost Entrene un clasificador. Use el clasificador. Identifique los casos que fueron mal clasificados. Construya un nuevo clasificador que clasifique mejor los casos mal clasificados del punto anterior. Repita los pasos 2 a 4 varias veces. Asígnele un peso a cada clasificador y júntelos para obtener un clasificador con mejor desempeño. Explicación detallada de AdaBoost Inicie con un conjunto de entrenamiento \\((X, Y)\\) con \\(m\\) observaciones denotadas como \\((x_1, y_1), \\ldots, (x_m, y_m)\\) de tal manera que \\(x_i \\in R^p\\). Los valores de \\(y\\) deben ser -1 o 1 para aplicar el método. Inicie con la distribución discreta \\(D_1(i)=1/m\\) que indica el peso de la observación \\(i\\) en la iteración \\(1\\). Para \\(t=1, \\ldots, T\\). Construya un clasificador \\(h_t\\) definido así: \\(h_t : X \\rightarrow \\{-1, 1 \\}\\). Calcule el error asociado \\(\\epsilon_t\\) al clasificador \\(\\epsilon_t= \\sum_{i=1}^m D_t(i) \\times \\delta_i\\), donde \\(\\delta_i=0\\) si \\(h_t(x_i)=y_i\\), es decir, si fue correcta la clasificación; caso contrario es \\(\\delta_i=1\\). Calcule la nueva distribución \\(D_{t+1}(i)=D_{t}(i) \\times F_i / Z_t\\), donde: \\(F_i=\\exp(-\\alpha_t)\\) si la clasificación fue correcta, es decir si \\(h_t(x_i) = y_i\\). \\(F_i=\\exp(\\alpha_t)\\) si la clasificación fue incorrecta, es decir si \\(h_t(x_i) \\neq y_i\\). \\(\\alpha_t=\\frac{1}{2} \\log \\left( \\frac{1-\\epsilon_t}{\\epsilon_t} \\right)\\). \\(Z_t\\) es una constante de normalización de tal manera que \\(\\sum_{i=1}^m D_t(i)=1\\). Usualmente es \\(\\sum D_{t}(i) \\times F_i\\). Construya el clasificador final \\(H_{final}\\) como el promedio ponderado de los \\(t\\) clasificadores \\(h_t\\), usando \\(H_{final}=sign(\\sum_t \\alpha_t h_t(x))\\). Ejemplo Abajo se presenta un video con una explicación detallada de lo que es AdaBoost. Ejemplo En este ejemplo se ilustra la forma de aplicar AdaBoost a un conjunto de datos bivariados para clasificar en dos clases: -1 y +1. Figure 5.1: Datos originales. En la siguiente figura se muestran 3 clasificadores (\\(h_1\\), \\(h_2\\) y \\(h_3\\)) sencillos o árboles de profundidad uno (tocones), que fueron creados de forma secuencial. Al observar \\(h1\\), se nota que él clasificó mal los + encerrados en círculos. Por esa razón, en la siguiente iteración esas observaciones mal clasificadas tuvieron un mayor peso o importancia en el nuevo clasificador \\(h_2\\), por eso es que esos símbolos + aparecen más grandes en la segunda figura. Al mirar el clasificador \\(h_2\\) se observa que logró clasificar bien esos + grandes, sin embargo, él clasificó mal los - que están encerrados en círculos. Por esa razón, en la siguiente iteración esas observaciones mal clasificadas tuvieron un mayor peso o importancia en el nuevo clasificador \\(h_3\\), por eso es que esos símbolos - aparecen más grandes en la tercera figura. El clasificador \\(h_3\\) logra clasificar mejor esos -. Figure 5.2: Clasificadores. El clasificador final (\\(h_{final}\\)) se construye como una ponderación de los clasificadores sencillos (\\(h_1\\), \\(h_2\\) y \\(h_3\\)) como se muestra a continuación. Figure 5.3: Obtención clasificador final. Los valores de \\(\\alpha_t\\) son las ponderaciones que aparecen en la explicación detallada del método. Ejemplo En este ejemplo vamos a usar la base de datos kyphosis del paquete rpart (Therneau and Atkinson 2019). A continuación las primeras líneas de la base de datos. library(rpart) head(kyphosis) ## Kyphosis Age Number Start ## 1 absent 71 3 5 ## 2 absent 158 3 14 ## 3 present 128 4 5 ## 4 absent 2 5 1 ## 5 absent 1 4 15 ## 6 absent 1 2 16 El objetivo es crear un clasificador que use la información de Age, Number y Start para predecir el tipo de deformación kyphosis (absent or present). Como vamos a aplicar el AdaBoost manual debemos crear la variable respuesta en -1 y 1 así: kyphosis$y &lt;- ifelse(kyphosis$Kyphosis == &#39;absent&#39;, -1, 1) Vamos a crear primero un modelo de referencia mod0 que será un árbol tradicional. mod0 &lt;- rpart(y ~ Age + Number + Start, data=kyphosis, method=&#39;class&#39;) y0 &lt;- predict(mod0, type=&#39;class&#39;) tabla0 &lt;- table(y0, kyphosis$y) sum(diag(tabla0)) / sum(tabla0) ## [1] 0.8395062 En el siguiente código se usarán 20 iteraciones del AdaBoost en forma manual para crear 20 tocones (stumps) que unidos nos permitirán clasificar. T &lt;- 20 # numero de iteraciones m &lt;- nrow(kyphosis) Dt &lt;- rep(1/m, m) # D1 alphas &lt;- numeric(T) # Para almacenar alpha y_hats &lt;- matrix(NA, ncol=T, nrow=m) # Para almacenar la predicciones for (i in 1:T) { mod &lt;- rpart(y ~ Age + Number + Start, weights=Dt, data=kyphosis, method=&#39;class&#39;, control=rpart.control(maxdepth = 1)) y_hat &lt;- predict(mod, type=&#39;class&#39;) error &lt;- ifelse(y_hat == kyphosis$y, 0, 1) # 1=error, 0=ok epsilon_t &lt;- sum(error * Dt) alpha_t &lt;- 0.5 * log((1-epsilon_t)/epsilon_t) Fi &lt;- ifelse(y_hat == kyphosis$y, exp(-alpha_t), exp(alpha_t)) Dt &lt;- Dt * Fi Dt &lt;- Dt / sum(Dt) alphas[i] &lt;- alpha_t y_hats[, i] &lt;- ifelse(y_hat == &#39;-1&#39;, -1, 1) } Para obtener las estimaciones con los 20 tocones se usa el siguiente código. y_final &lt;- c(sign(y_hats %*% matrix(alphas, ncol=1))) tabla_final &lt;- table(y_final, kyphosis$y) sum(diag(tabla_final)) / sum(tabla_final) ## [1] 0.8888889 Por último vamos a usar la función boosting del paquete adabag (Alfaro et al. 2018) para aplicar AdaBoost de forma automática. library(adabag) adaboost &lt;- bagging(Kyphosis ~ Age + Number + Start, coeflearn=&#39;Freund&#39;, data=kyphosis, mfinal=20) yhat_adaboost &lt;- predict(adaboost, newdata=kyphosis)$class tabla_adaboost &lt;- table(yhat_adaboost, kyphosis$Kyphosis) sum(diag(tabla_adaboost)) / sum(tabla_adaboost) ## [1] 0.8271605 Abajo se muestra nuevamente el código para calcular las tres tasas de clasificación correcta para el modelo de referencia, el AdaBoost manual y el AdaBoost automático. sum(diag(tabla0)) / sum(tabla0) ## [1] 0.8395062 sum(diag(tabla_final)) / sum(tabla_final) ## [1] 0.8888889 sum(diag(tabla_adaboost)) / sum(tabla_adaboost) ## [1] 0.8271605 De la salida anterior se observa que el árbol tradicional tiene tasa de clasificación correcta menor que las dos versiones de AdaBoost. Ejemplo En este ejemplo vamos a mostrar como usar el paquete adabag (Alfaro et al. 2018) para aplicar AdaBoost. Como ejemplo vamos a usar la base de datos iris en la cual se tienen 4 variables que ayudarán a clasificar nuevas flores en una de tres especies. En la figura de abajo se muestran las tres especies y las cuatro variables. Figure 5.4: Tipos de especies y variables en la base de datos iris. Primero vamos a crear un árbol de clasificación usando el paquete rpart (Therneau and Atkinson 2019), este modelo servirá como modelo de referencia. Para ajustar este modelo sólo vamos a usar las variables Sepal.Length y Sepal.Width . library(rpart) mod1 &lt;- rpart(Species ~ Sepal.Length + Sepal.Width, data=iris) yhat1 &lt;- predict(mod1, type=&#39;class&#39;) Ahora vamos a aplicar AdaBoost. library(adabag) mod2 &lt;- bagging(Species ~ Sepal.Length + Sepal.Width, data=iris, mfinal=10) yhat2 &lt;- predict(mod2, newdata=iris)$class En los objetos yhat1 y yhat2 están las predicciones y con ellas vamos a formar las tablas de confusión para cada modelo. t1 &lt;- table(real=iris$Species, prediccion=yhat1) t1 ## prediccion ## real setosa versicolor virginica ## setosa 49 1 0 ## versicolor 3 31 16 ## virginica 0 11 39 t2 &lt;- table(real=iris$Species, prediccion=yhat2) t2 ## prediccion ## real setosa versicolor virginica ## setosa 49 1 0 ## versicolor 2 38 10 ## virginica 1 16 33 Ahora vamos a calcular la tasa de clasificación correcta para cada una de las tablas anteriores. sum(diag(t1)) / sum(t1) ## [1] 0.7933333 sum(diag(t2)) / sum(t2) ## [1] 0.8 De la anterior salida se observa que la tasa de clasificación correcta con un árbol tradicional es 79.3333333% mientras que con AdaBoost es 80%. References "],
["gradboost.html", "6 Gradient Boost Ejemplo Ejemplo Ejemplo", " 6 Gradient Boost Gradient Boost fue propuesto por (Friedman 1999a) y (Friedman 1999b) y consiste en crear varios predictores en secuencia. El primer predictor usa la media de la variable \\(Y\\) para predecir, luego el segundo predictor explica los errores del primer predictor, el tercer predictor explicar los erroes del segundo predictor y así sucesivamente. En la siguiente figura se muestra una ilustración de lo que es Gradient Boost. Ejemplo Abajo se presenta un video con una explicación detallada de lo que es Gradient Boost. Ejemplo En este ejemplo se muestra como aplicar Gradient Boost de forma manual usando los datos del video anterior. height &lt;- c(1.6, 1.6, 1.5, 1.8, 1.5, 1.4) color &lt;- c(&quot;blue&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;green&quot;, &quot;blue&quot;) gender &lt;- c(&quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;) weigth &lt;- c(88, 76, 56, 73, 77, 57) La librería para crear los árboles será rpart. library(rpart) El valor learning rate en el ejemplo será \\(\\alpha=0.10\\). A continuación el código para crear el modelo inicial y los modelos siguientes del video anterior. a &lt;- 0.1 # Learning rate # Modelo inicial res1 &lt;- weigth - mean(weigth) # Modelo 2 mod2 &lt;- rpart(res1 ~ height + color + gender, control=rpart.control(minsplit = 3)) res2 &lt;- weigth - (mean(weigth) + a * predict(mod2)) # Modelo 3 mod3 &lt;- rpart(res2 ~ height + color + gender, control=rpart.control(minsplit = 3)) res3 &lt;- weigth - (mean(weigth) + a * predict(mod2) + a * predict(mod3)) A continuación una tabla con los residuales de los modelos. cbind(res1, res2, res3) ## res1 res2 res3 ## 1 16.833333 15.15 13.635 ## 2 4.833333 4.35 3.915 ## 3 -15.166667 -13.70 -12.380 ## 4 1.833333 1.45 1.105 ## 5 5.833333 5.45 5.105 ## 6 -14.166667 -12.70 -11.380 Para predecir el valor de weigth cuando height=1.7, color=“green” y gender=“male” se usa el siguiente código. new_data &lt;- data.frame(height=1.7, color=&quot;green&quot;, gender=&quot;female&quot;) mean(weigth) + a * predict(mod2, new_data) + a * predict(mod3, new_data) ## 1 ## 72.085 Ejemplo En este ejemplo se van a usar los datos Boston del paquete (Ripley 2020) para predecir la variable medv en función de las otras covariables. Para explorar la base de datos usamos el siguiente código. library(MASS) str(Boston) ## &#39;data.frame&#39;: 506 obs. of 14 variables: ## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... ## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... ## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... ## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... ## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... ## $ rm : num 6.58 6.42 7.18 7 7.15 ... ## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... ## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... ## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... ## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... ## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... ## $ black : num 397 397 393 395 397 ... ## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... ## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... Vamos a crear dos conjuntos de datos, uno de entrenamiento y otro de validación de la siguiente manera. library(caret) set.seed(123) indexes &lt;- createDataPartition(Boston$medv, p = .90, list = F) train &lt;- Boston[indexes, ] test &lt;- Boston[-indexes, ] El primer modelo predictivo será un árbol sencillo que nos servirá como elemento de comparación. library(rpart) model_tree &lt;- rpart(medv ~ ., data=train) Ahora vamos a calcular el \\(RMSE\\) (root mean square error) y la correlación entre \\(y\\) y \\(\\hat{y}\\). y_hat &lt;- predict(model_tree) sqrt(sum((train$medv - y_hat)^2)) ## [1] 83.63843 cor(train$medv, y_hat) ## [1] 0.9070587 Para comparar los resultados del modelo vamos a crear un diagrama de dispersión entre \\(y\\) y \\(\\hat{y}\\). plot(x=train$medv, y=y_hat, xlab=&#39;Valor observado&#39;, ylab=&#39;Predicción&#39;) abline(a=0, b=1, lty=&#39;dashed&#39;) Ahora vamos a usar la función gbm del paquete (Greenwell et al. 2020) para aplicar el algoritmo Gradient Boost con \\(\\alpha=0.01\\), 5000 árboles (iteraciones) y una profundidad de 1 en cada árbol (stump). library(gbm) set.seed(123) # for reproducibility model_gbm1 &lt;- gbm(medv ~., data = train, distribution=&quot;gaussian&quot;, cv.folds=5, shrinkage=0.01, n.minobsinnode=10, n.trees=5000, interaction.depth=1) print(model_gbm1) ## gbm(formula = medv ~ ., distribution = &quot;gaussian&quot;, data = train, ## n.trees = 5000, interaction.depth = 1, n.minobsinnode = 10, ## shrinkage = 0.01, cv.folds = 5) ## A gradient boosted model with gaussian loss function. ## 5000 iterations were performed. ## The best cross-validation iteration was 4404. ## There were 13 predictors of which 13 had non-zero influence. Ahora vamos a calcular el \\(RMSE\\) (root mean square error) y la correlación entre \\(y\\) y \\(\\hat{y}\\). sqrt(min(model_gbm1$cv.error)) ## [1] 3.636371 y_hat &lt;- predict(model_gbm1) ## Using 4404 trees... cor(train$medv, y_hat) ## [1] 0.9542746 Para comparar los resultados del modelo vamos a crear un diagrama de dispersión entre \\(y\\) y \\(\\hat{y}\\). plot(x=train$medv, y=y_hat, xlab=&#39;Valor observado&#39;, ylab=&#39;Predicción&#39;) abline(a=0, b=1, lty=&#39;dashed&#39;) Al comparar la correlación y los diagramas de dispersión se observa una mejora considerable con el modelo Gradient Boost. Podemos construir una figura para observar la evolución del \\(RMSE\\) en función del número de árboles (iteraciones). gbm.perf(model_gbm1, method = &quot;cv&quot;) ## [1] 4404 De la figura anterior se observa que en la iteración 4404 (linea azul a trazos) fue donde se obtuvo el menor \\(RMSE\\). Vamos a crear otro modelo de predicción pero cambiando algunos de los hiper-parámetros, \\(\\alpha=0.15\\) y una profundidad de 3 en cada árbol. set.seed(123) model_gbm2 &lt;- gbm(medv ~., data = train, distribution=&quot;gaussian&quot;, cv.folds=5, shrinkage=0.15, n.minobsinnode=10, n.trees=5000, interaction.depth=3) Ahora vamos a calcular el \\(RMSE\\) (root mean square error) y la correlación entre \\(y\\) y \\(\\hat{y}\\). sqrt(min(model_gbm2$cv.error)) ## [1] 3.370139 y_hat &lt;- predict(model_gbm2) ## Using 241 trees... cor(train$medv, y_hat) ## [1] 0.9852213 Para comparar los resultados del modelo vamos a crear un diagrama de dispersión entre \\(y\\) y \\(\\hat{y}\\). plot(x=train$medv, y=y_hat, xlab=&#39;Valor observado&#39;, ylab=&#39;Predicción&#39;) abline(a=0, b=1, lty=&#39;dashed&#39;) Podemos construir una figura para observar la evolución del \\(RMSE\\) en función del número de árboles (iteraciones). gbm.perf(model_gbm2, method = &quot;cv&quot;) ## [1] 241 En lugar de buscar esos hiper-parámetros a manualmente, podemos hacer una búsqueda automática creando un conjunto de posibles valores de los hiper-parámetros así. # create hyperparameter grid hyper_grid &lt;- expand.grid( shrinkage = c(0.01, 0.1, 0.3), interaction.depth = c(1, 3, 5), n.minobsinnode = c(5, 10, 15), bag.fraction = c(0.65, 0.8, 1), optimal_trees = 0, # a place to dump results min_RMSE = 0, # a place to dump results min_cor = 0 ) nrow(hyper_grid) # total number of combinations ## [1] 81 Para hacer la búsqueda usamos el siguiente código. # randomize data random_index &lt;- sample(1:nrow(train), nrow(train)) random_train &lt;- train[random_index, ] for(i in 1:nrow(hyper_grid)) { set.seed(123) gbm.tune &lt;- gbm( formula = medv ~ ., distribution = &quot;gaussian&quot;, data = random_train, n.trees = 5000, interaction.depth = hyper_grid$interaction.depth[i], shrinkage = hyper_grid$shrinkage[i], n.minobsinnode = hyper_grid$n.minobsinnode[i], bag.fraction = hyper_grid$bag.fraction[i], train.fraction = 0.75, n.cores = NULL, # will use all cores by default verbose = FALSE ) # agregando la inf que nos interesa hyper_grid$optimal_trees[i] &lt;- which.min(gbm.tune$valid.error) hyper_grid$min_RMSE[i] &lt;- sqrt(min(gbm.tune$valid.error)) hyper_grid$min_cor[i] &lt;- cor(random_train$medv, predict(gbm.tune)) } Organizamos los resultados en relación al menor \\(RMSE\\). library(dplyr) hyper_grid %&gt;% dplyr::arrange(min_RMSE) %&gt;% head(10) ## shrinkage interaction.depth n.minobsinnode bag.fraction optimal_trees ## 1 0.30 5 5 0.80 88 ## 2 0.30 5 5 0.65 127 ## 3 0.01 5 5 0.80 3019 ## 4 0.01 3 5 0.65 4897 ## 5 0.01 5 5 0.65 4137 ## 6 0.10 5 5 1.00 128 ## 7 0.01 3 5 0.80 4884 ## 8 0.01 5 5 1.00 2543 ## 9 0.01 3 10 0.80 4984 ## 10 0.01 5 10 0.80 3699 ## min_RMSE min_cor ## 1 2.685273 0.9862784 ## 2 2.729782 0.9869481 ## 3 2.778927 0.9869945 ## 4 2.782291 0.9853981 ## 5 2.796934 0.9876546 ## 6 2.800982 0.9803835 ## 7 2.837197 0.9852038 ## 8 2.837520 0.9855254 ## 9 2.860405 0.9838807 ## 10 2.870885 0.9860587 # for reproducibility set.seed(123) # train GBM model gbm.fit.final &lt;- gbm( formula = medv ~ ., distribution = &quot;gaussian&quot;, data = train, n.trees = 88, interaction.depth = 5, shrinkage = 0.3, n.minobsinnode = 5, bag.fraction = 0.80, train.fraction = 1, n.cores = NULL, # will use all cores by default verbose = FALSE ) summary(gbm.fit.final, cBars = 10, method = relative.influence, las = 2) ## var rel.inf ## rm rm 40.70059606 ## lstat lstat 36.67225822 ## dis dis 8.03512644 ## crim crim 4.10937068 ## nox nox 3.99034366 ## ptratio ptratio 2.13830279 ## age age 1.34811594 ## black black 1.16184689 ## tax tax 1.14437416 ## indus indus 0.37045656 ## rad rad 0.21608957 ## zn zn 0.07111692 ## chas chas 0.04200211 http://uc-r.github.io/gbm_regression y_hat &lt;- predict(object=gbm.fit.final, newdata=test, n.trees = 88) cor(test$medv, y_hat) ## [1] 0.934735 plot(x=test$medv, y=y_hat, xlab=&#39;Valor observado&#39;, ylab=&#39;Predicción&#39;) abline(a=0, b=1, lty=&#39;dashed&#39;) References "],
["reg-versus-arb.html", "7 Regresión lineal versus árboles de regresión Regresión lineal Arboles de regresión Ejemplo Estudio de simulación para comparar ambos métodos Retos", " 7 Regresión lineal versus árboles de regresión En este capítulo se muestra una comparación entre modelos de regresión y árboles de regresion. Regresión lineal El modelo de regresión lineal simple es uno de los más populares en modelación. Este modelo se puede resumir a continuación. \\[\\begin{align} y_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 x_i, \\\\ \\sigma^2 &amp;= \\text{constante} \\end{align}\\] Arboles de regresión Una explicación de los árboles de regresión puede ser consultada en el capítulo 1. Las librerías en R para implementar árboles de regresión son: library(rpart) library(rpart.plot) Ejemplo Como ilustración vamos a usar los datos del ejemplo 2.1 del libro de Montgomery, Peck and Vining (2003). En el ejemplo 2.1 los autores ajustaron un modelo de regresión lineal simple para explicar la Resistencia de una soldadura en función de la Edad de la misma. A continuación el código para cargar los datos y una muestra de las 6 primeras observaciones de la base de datos, en total tenemos 20 observaciones. file &lt;- &quot;https://raw.githubusercontent.com/fhernanb/datos/master/propelente&quot; datos &lt;- read.table(file=file, header=TRUE) head(datos) # shows the first 6 rows ## Resistencia Edad ## 1 2158.70 15.50 ## 2 1678.15 23.75 ## 3 2316.00 8.00 ## 4 2061.30 17.00 ## 5 2207.50 5.50 ## 6 1708.30 19.00 Para crear un diagrama de dispersión que nos muestre la relación entre las dos variables usamos las siguientes instrucciones. library(ggplot2) ggplot(datos, aes(x=Edad, y=Resistencia)) + geom_point() + theme_light() De la figura anterior se ve claramente que a medida que aumenta la edad de la soldadura, la resistencia que ella ofrece disminuye. Adicionalmente, se observa que la relación entre las variables es lineal con una dispersión que parece constante. ¿Quién estima mejor? ¿un modelo de regresión lineal simple o un árbol? rls &lt;- lm(Resistencia ~ Edad, data=datos) arb &lt;- rpart(Resistencia ~ Edad, data=datos) arb &lt;- rpart(Resistencia ~ Edad, data=datos, method=&quot;anova&quot;) ¿Qué hay dentro de modelo de regresión lineal simple? summary(rls) ## ## Call: ## lm(formula = Resistencia ~ Edad, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -215.98 -50.68 28.74 66.61 106.76 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2627.822 44.184 59.48 &lt; 2e-16 *** ## Edad -37.154 2.889 -12.86 1.64e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 96.11 on 18 degrees of freedom ## Multiple R-squared: 0.9018, Adjusted R-squared: 0.8964 ## F-statistic: 165.4 on 1 and 18 DF, p-value: 1.643e-10 ¿Qué hay dentro de modelo del arbol? summary(arb) ## Call: ## rpart(formula = Resistencia ~ Edad, data = datos, method = &quot;anova&quot;) ## n= 20 ## ## CP nsplit rel error xerror xstd ## 1 0.7480619 0 1.0000000 1.184195 0.250625 ## 2 0.0100000 1 0.2519381 1.184195 0.250625 ## ## Variable importance ## Edad ## 100 ## ## Node number 1: 20 observations, complexity param=0.7480619 ## mean=2131.358, MSE=84686.88 ## left son=2 (8 obs) right son=3 (12 obs) ## Primary splits: ## Edad &lt; 16.25 to the right, improve=0.7480619, (0 missing) ## ## Node number 2: 8 observations ## mean=1823.094, MSE=19439.95 ## ## Node number 3: 12 observations ## mean=2336.867, MSE=22599.79 Construyamos nuevamente el árbol pero explorando todas las opciones de la función prp. prp(arb, main = &quot;&quot;, nn = TRUE, # display the node numbers fallen.leaves = TRUE, # put the leaves on the bottom of the page shadow.col = &quot;gray&quot;, # shadows under the leaves branch.lty = 3, # draw branches using dotted lines branch = .5, # change angle of branch lines faclen = 0, # faclen = 0 to print full factor names trace = 1, # print the auto calculated cex, xlim, ylim split.cex = 1.2, # make the split text larger than the node text split.prefix = &quot;is &quot;, # put &quot;is &quot; before split text split.suffix = &quot;?&quot;, # put &quot;?&quot; after split text split.box.col = &quot;lightblue&quot;, # lightgray split boxes (default is white) split.border.col = &quot;darkgray&quot;, # darkgray border on split boxes split.round = 0.5) # round the split box corners a tad ## cex 1 xlim c(-0.65, 1.65) ylim c(-0.15, 1.15) A continuación las predicciones con ambos modelos. pred_rls &lt;- predict(object=rls, newdata=datos) pred_arb &lt;- predict(object=arb, newdata=datos) Dibujemos \\(y_i\\) versus \\(\\hat{y}_i\\). par(mfrow=c(1, 2)) plot(x=pred_rls, y=datos$Resistencia, main=&quot;RLS&quot;) abline(a=0, b=1, lty=&quot;dashed&quot;, col=&quot;blue&quot;) plot(x=pred_arb, y=datos$Resistencia, main=&quot;Arbol&quot;) abline(a=0, b=1, lty=&quot;dashed&quot;, col=&quot;blue&quot;) Vamos a calcular \\(Cor(y_i, \\hat{y}_i)\\). cor(datos$Resistencia, pred_rls) ## [1] 0.9496533 cor(datos$Resistencia, pred_arb) ## [1] 0.8649057 Calculemos ahora el Error Cuadrático Medio \\(ECM=\\frac{1}{n}\\sum(y_i-\\hat{y}_i)^2\\). mean((datos$Resistencia - pred_rls)^2) ## [1] 8312.743 mean((datos$Resistencia - pred_arb)^2) ## [1] 21335.85 ¿Cuál método prefiere usted? Estudio de simulación para comparar ambos métodos El objetivo es comparar ambos modelos repetidas veces, para esto vamos a simular conjuntos de datos que tengan un comportamiento lineal y parecido a los datos del ejemplo. El modelo que vamos a considerar es el siguiente: \\[\\begin{align} y_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= 2627 - 37 x_i, \\\\ \\sigma &amp;= 96, \\\\ x &amp;\\sim U(2, 25) \\end{align}\\] Vamos a crear una función generadora de datos. gen_dat &lt;- function(n) { x &lt;- runif(n=n, min=2, max=25) media &lt;- 2627 - 37 * x y &lt;- rnorm(n=n, mean=media, sd=96) data.frame(x=x, y=y) } Generemos unos datos de prueba y graficamos los datos. datos_train &lt;- gen_dat(n=20) ggplot(datos_train, aes(x=x, y=y)) + geom_point() + theme_light() Usando los datos de prueba vamos a ajustar los modelos y luego calcularemos los indicadores. datos_train &lt;- gen_dat(n=20) # Para entrenar datos_test &lt;- gen_dat(n=20) # Para validar rls &lt;- lm(y ~ x, data=datos_train) arb &lt;- rpart(y ~ x, data=datos_train) pred_rls &lt;- predict(object=rls, newdata=datos_test) pred_arb &lt;- predict(object=arb, newdata=datos_test) cor(datos_test$y, pred_rls) ## [1] 0.9531806 cor(datos_test$y, pred_arb) ## [1] 0.8176202 mean((datos_test$y - pred_rls)^2) ## [1] 7951.993 mean((datos_test$y - pred_arb)^2) ## [1] 32835.24 Al observar los resultados anteriores vemos que el modelo de regresión lineal se comporta mejor que el árbol de regresión, esto se debe a que los datos están siendo generados con un modelo de regresión lineal. Ahora vamos a realizar el estudio de simulación para explorar el efecto de \\(n = 10, 20, 40\\) sobre el \\(ECM\\) usando 5 réplicas para cada \\(n\\), este es un estudio de simulación “naive” pero ilustrativo. n &lt;- c(10, 20, 40) nrep &lt;- 5 result &lt;- numeric() for (i in n) { for(k in 1:nrep) { datos_train &lt;- gen_dat(n=i) # Para entrenar datos_test &lt;- gen_dat(n=i) # Para validar rls &lt;- lm(y ~ x, data=datos_train) arb &lt;- rpart(y ~ x, data=datos_train) pred_rls &lt;- predict(object=rls, newdata=datos_test) pred_arb &lt;- predict(object=arb, newdata=datos_test) ecm1 &lt;- mean((datos_test$y - pred_rls)^2) ecm2 &lt;- mean((datos_test$y - pred_arb)^2) result &lt;- rbind(result, c(i, ecm1, ecm2)) # No eficiente pero sirve } } colnames(result) &lt;- c(&quot;n&quot;, &quot;ecm_lrs&quot;, &quot;ecm_arb&quot;) result &lt;- as.data.frame(result) result ## n ecm_lrs ecm_arb ## 1 10 11561.409 45829.58 ## 2 10 5941.793 43297.25 ## 3 10 25397.007 111663.54 ## 4 10 12409.682 71720.23 ## 5 10 15095.712 44398.16 ## 6 20 10160.895 22593.85 ## 7 20 7179.949 28906.90 ## 8 20 6170.543 39031.82 ## 9 20 11935.307 28125.38 ## 10 20 11718.410 22418.01 ## 11 40 9400.226 19430.35 ## 12 40 6962.714 23450.19 ## 13 40 10313.839 20534.37 ## 14 40 8308.219 11837.12 ## 15 40 7127.208 19337.81 El objeto result tiene los resultados de la simulación, vamos a calcular el \\(ECM\\) promedio para rls y árboles diferenciando por \\(n\\). library(dplyr) result %&gt;% group_by(n) %&gt;% summarise(ecm_medio_lrs=mean(ecm_lrs), ecm_medio_arb=mean(ecm_arb)) ## # A tibble: 3 x 3 ## n ecm_medio_lrs ecm_medio_arb ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 14081. 63382. ## 2 20 9433. 28215. ## 3 40 8422. 18918. Retos A continuación los retos que usted debe aceptar. Extienda el estudio de simulación para otros valores de \\(n\\) y aumentando el número de repeticiones nrep, decida usted los valores. Con los resultados anteriores haga un gráfico de \\(ECM\\) promedio versus \\(n\\) para rls y árboles en la misma figura. ¿Se iguala \\(ECM\\) promedio del árbol con el de regresión para algún valor de \\(n\\)? ¿Cuál técnica presenta el \\(ECM\\) menor? ¿Es posible encontrar un \\(ECM=0\\) para algún valor de \\(n\\)? ¿Para qué sirve el paquete dplyr? ¿Qué es un tibble? "],
["references.html", "References", " References "]
]
